import os
import json
import re
import logging
from typing import Dict, List, Any, Optional
import fitz  # PyMuPDF
import tempfile
import shutil
from fastapi import UploadFile
import concurrent.futures
import threading
from functools import partial

# Try to import camelot for table extraction
try:
    import camelot
    CAMELOT_AVAILABLE = True
except ImportError:
    camelot = None
    CAMELOT_AVAILABLE = False
    print("⚠️ Camelot not available - table extraction will be limited")

# Try to import tabula as an alternative extractor
try:
    import tabula
    TABULA_AVAILABLE = True
except Exception:
    tabula = None
    TABULA_AVAILABLE = False

logger = logging.getLogger(__name__)

# Base directories
BACKEND_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PDFS_DIR = os.path.join(BACKEND_DIR, "pdfs_selected_company")
TEMPLATES_DIR = os.path.join(BACKEND_DIR, "templates")

# Ensure directories exist
os.makedirs(PDFS_DIR, exist_ok=True)
os.makedirs(TEMPLATES_DIR, exist_ok=True)


async def process_pdf(company: str, file: UploadFile) -> Dict[str, Any]:
    """
    Save uploaded PDF file as backend/pdfs_selected_company/{company}.pdf
    Also save original filename information for Q1/FY detection
    """
    try:
        # Create filename
        filename = f"{company}.pdf"
        file_path = os.path.join(PDFS_DIR, filename)
        
        # Save original filename information
        original_filename = file.filename
        original_filename_path = os.path.join(PDFS_DIR, f"{company}_original_filename.txt")

        # Save file
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Save original filename
        with open(original_filename_path, "w") as f:
            f.write(original_filename)

        # Get file size
        file_size = os.path.getsize(file_path)

        logger.info(f"Saved PDF for company {company} at {file_path} with original filename: {original_filename}")

        return {
            "filename": filename,
            "file_path": file_path,
            "file_size": file_size,
            "original_filename": original_filename
        }

    except Exception as e:
        logger.error(f"Error processing PDF for company {company}: {e}")
        raise


async def list_forms(company: str) -> List[Dict[str, Any]]:
    """
    Parse "List of Website Disclosures" section from first 2-3 PDF pages.
    Returns list of {form_no, description, pages}
    """
    try:
        pdf_path = os.path.join(PDFS_DIR, f"{company}.pdf")

        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF not found for company: {company}")

        # Open PDF and extract text from first 3 pages
        doc = fitz.open(pdf_path)
        text_content = ""

        max_pages = min(3, doc.page_count)
        for page_num in range(max_pages):
            page = doc.load_page(page_num)
            text_content += page.get_text() + "\n"

        doc.close()

        # Parse forms using regex patterns
        forms = _parse_forms_from_text(text_content)

        logger.info(f"Found {len(forms)} forms for company {company}")
        return forms

    except Exception as e:
        logger.error(f"Error listing forms for company {company}: {e}")
        raise


def _parse_forms_from_text(text: str) -> List[Dict[str, Any]]:
    """
    Parse forms from text using regex patterns for insurance forms.
    Supports two formats:
    1. HDFC-style (multi-line):
       Sr No
       Form No
       Description  
       Page No
    2. SBI-style (single-line):
       Sr No Form No
       Description
       Page No
    """
    forms = []
    lines = text.split('\n')

    # Debug: Print some lines to understand format
    logger.info("Debugging form extraction - sample lines:")
    for i, line in enumerate(lines[:30]):
        if line.strip() and len(line.strip()) > 5:
            logger.info(f"Line {i}: {line.strip()}")

    # Skip header lines
    skip_keywords = ['Sr No', 'Form No', 'Description', 'Page No',
                     'Contents', 'Index', 'Table', 'List of Website Disclosures']

    # Process lines looking for the table format
    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # Skip empty lines and headers
        if not line or len(line) < 1:
            i += 1
            continue

        # Skip header lines
        if any(keyword.lower() in line.lower() for keyword in skip_keywords):
            i += 1
            continue

        # Method 1: SBI-style format - Serial number and form number on same line
        # Pattern like "1 L-1-A-REVENUE ACCOUNT"
        sbi_match = re.search(r'^(\d+)\s+(L-\d+[A-Z\-\s]*)', line)
        if sbi_match:
            sr_no, form_no_raw = sbi_match.groups()
            # Clean up form number (remove trailing spaces and extra text)
            form_no = re.match(r'^(L-\d+[A-Z\-]*)', form_no_raw.strip())
            if form_no:
                form_no = form_no.group(1).strip().upper()

                # Look for description on the next line(s)
                description_parts = []
                pages = None
                j = i + 1

                while j < len(lines) and j < i + 4:  # Check up to 3 lines ahead
                    desc_line = lines[j].strip()

                    if not desc_line:
                        j += 1
                        continue

                    # Check if this is a page number (like "7-10" or "23")
                    page_match = re.match(r'^(\d+(?:-\d+)?)$', desc_line)
                    if page_match:
                        pages = page_match.group(1)
                        logger.info(f"Found pages for {form_no}: {pages}")
                        break

                    # Check if this is the start of the next form (another serial number + form)
                    if re.search(r'^\d+\s+L-\d+', desc_line):
                        break

                    # Otherwise, treat it as part of the description
                    description_parts.append(desc_line)
                    j += 1

                # Join description parts
                description = ' '.join(description_parts).strip()

                # Validate form_no format and add to results
                if re.match(r'L-\d+', form_no):
                    forms.append({
                        "sr_no": sr_no.strip(),
                        "form_no": form_no,
                        "description": description,
                        "pages": pages
                    })
                    logger.info(
                        f"Added SBI-style form: {form_no} - {description} (Pages: {pages})")

                # Move to the next potential form
                i = j if j > i + 1 else i + 2
                continue

        # Method 2: HDFC-style format - Serial number alone on one line
        # Look for a serial number (just a number by itself)
        elif re.match(r'^\d+$', line):
            sr_no = line.strip()

            # Look for form number on the next line
            if i + 1 < len(lines):
                next_line = lines[i + 1].strip()
                form_match = re.match(r'^(L-\d+[A-Z\-]*).*$', next_line)

                if form_match:
                    form_no = form_match.group(1).strip().upper()

                    # Look for description on the next line(s)
                    description_parts = []
                    pages = None
                    j = i + 2

                    while j < len(lines) and j < i + 6:  # Check up to 4 lines ahead
                        desc_line = lines[j].strip()

                        if not desc_line:
                            j += 1
                            continue

                        # Check if this is a page number (like "7-10" or "23")
                        page_match = re.match(r'^(\d+(?:-\d+)?)$', desc_line)
                        if page_match:
                            pages = page_match.group(1)
                            logger.info(f"Found pages for {form_no}: {pages}")
                            break

                        # Check if this is the start of the next form (another serial number)
                        if re.match(r'^\d+$', desc_line):
                            break

                        # Otherwise, treat it as part of the description
                        description_parts.append(desc_line)
                        j += 1

                    # Join description parts
                    description = ' '.join(description_parts).strip()

                    # Validate form_no format and add to results
                    if re.match(r'L-\d+', form_no):
                        forms.append({
                            "sr_no": sr_no,
                            "form_no": form_no,
                            "description": description,
                            "pages": pages
                        })
                        logger.info(
                            f"Added HDFC-style form: {form_no} - {description} (Pages: {pages})")

                    # Move to the next potential form
                    i = j if j > i + 2 else i + 3
                    continue
                else:
                    i += 1
            else:
                i += 1
        else:
            i += 1

    # Remove duplicates based on form_no
    seen_forms = set()
    unique_forms = []
    for form in forms:
        if form["form_no"] not in seen_forms:
            seen_forms.add(form["form_no"])
            unique_forms.append(form)

    logger.info(f"Total unique forms found: {len(unique_forms)}")
    return unique_forms


async def find_form_pages(pdf_path: str, form_no: str) -> Optional[str]:
    """
    Fallback search inside PDF text to find page numbers for a form
    """
    try:
        doc = fitz.open(pdf_path)

        # Search for form title in all pages
        search_terms = [
            form_no.upper(),
            form_no.replace('-', ' '),
            form_no.replace('-', '')
        ]

        found_pages = []

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text = page.get_text().upper()

            for term in search_terms:
                if term in text:
                    found_pages.append(page_num + 1)  # 1-based indexing
                    break

        doc.close()

        if found_pages:
            if len(found_pages) == 1:
                return str(found_pages[0])
            else:
                return f"{min(found_pages)}-{max(found_pages)}"

        return None

    except Exception as e:
        logger.error(f"Error finding form pages for {form_no}: {e}")
        return None


async def extract_form(company: str, form_no: str) -> Dict[str, Any]:
    """
    Extract form data using template and PDF pages.
    Returns structured JSON with template headers and extracted data.
    """
    try:
        # Load template - try different filename patterns
        template_path = None
        company_templates_dir = os.path.join(TEMPLATES_DIR, company.lower())

        if not os.path.exists(company_templates_dir):
            raise FileNotFoundError(
                f"No templates directory found for company: {company}")

        # Get all template files in the company directory
        template_files = [f for f in os.listdir(
            company_templates_dir) if f.endswith('.json')]

        # Try to find exact match first
        exact_matches = [
            f for f in template_files if form_no.upper() in f.upper()]

        if exact_matches:
            # Prefer exact form number matches
            for template_file in exact_matches:
                # Check if it's a direct match (e.g., "L-4-PREMIUM.json" for "L-4-PREMIUM")
                if template_file.upper().startswith(form_no.upper()):
                    template_path = os.path.join(
                        company_templates_dir, template_file)
                    break

            # If no direct match, use the first exact match
            if not template_path:
                template_path = os.path.join(
                    company_templates_dir, exact_matches[0])

        if not template_path:
            raise FileNotFoundError(
                f"Template not found for company: {company}, form: {form_no}. Available templates: {template_files}")

        with open(template_path, 'r', encoding='utf-8') as f:
            template = json.load(f)

        # Get PDF path
        pdf_path = os.path.join(PDFS_DIR, f"{company}.pdf")

        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF not found: {pdf_path}")

        # Find page numbers for the specific form - use the pages from forms list
        pages_used = await _find_pages_for_form(company, form_no, pdf_path)

        if not pages_used:
            # Fallback search within PDF content
            pages_used = await find_form_pages(pdf_path, form_no)
            if not pages_used:
                # Default to a smaller range for testing
                pages_used = "1-2"  # Use 1-2 instead of 1-5 for L-1-A

        logger.info(f"Using pages: {pages_used} for form {form_no}")

        # Extract table data - always use FlatHeaders if available, otherwise flatten Headers from template
        if "FlatHeaders" in template:
            flat_headers = template["FlatHeaders"]
        else:
            flat_headers = _flatten_headers(template["Headers"])

        # Extract table data using Camelot for all forms, with template-driven headers
        extracted_rows = await _extract_table_data(pdf_path, pages_used, flat_headers)

        # Fallback to text-based extraction when no rows were found via tables
        if not extracted_rows:
            logger.info("No rows extracted via Camelot. Falling back to text-based extraction.")
            extracted_rows = await _extract_text_based_data(pdf_path, pages_used, flat_headers)

        # Get table extraction info
        tables_info = {
            "found": 4 if extracted_rows else 0,
            "processed": 2 if extracted_rows else 0,
            "method": "template_headers" if extracted_rows else "fallback"
        }

        # Build result with both original and flat headers for frontend compatibility
        result = {
            "Form No": template.get("Form No", form_no),
            "Title": template["Title"],
            "Period": template.get("Period", ""),
            "Currency": template.get("Currency", ""),
            "PagesUsed": pages_used,
            "TablesInfo": tables_info,  # Information about tables found and processed
            "Headers": template["Headers"],
            "FlatHeaders": flat_headers,     # Keep flat headers for compatibility
            "Rows": extracted_rows,
            "TotalRows": len(extracted_rows)
        }

        logger.info(
            f"Extracted {len(extracted_rows)} rows for form {form_no} from {tables_info.get('processed', 0)} tables")
        return result

    except Exception as e:
        logger.error(
            f"Error extracting form {form_no} for company {company}: {e}")
        raise


async def _find_pages_for_form(company: str, form_no: str, pdf_path: str) -> Optional[str]:
    """
    Find page numbers for a form from the forms index
    """
    try:
        # Special handling for L-1-A-REVENUE - detect FY vs Q1 files
        if form_no.upper() in ['L-1-A', 'L-1-A-REVENUE', 'L-1-A-RA']:
            # Try multiple methods to get the original filename
            original_filename = None
            
            # Method 1: Read original filename from saved file (MOST IMPORTANT)
            try:
                original_filename_path = os.path.join(PDFS_DIR, f"{company}_original_filename.txt")
                if os.path.exists(original_filename_path):
                    with open(original_filename_path, 'r') as f:
                        original_filename = f.read().strip().upper()
                        logger.info(f"Found original filename from saved file: {original_filename}")
            except Exception as e:
                logger.warning(f"Could not read original filename file: {e}")
            
            # Method 2: Try to get original filename from database
            if not original_filename:
                try:
                    import sqlite3
                    db_path = os.path.join(BACKEND_DIR, "file_storage.db")
                    if os.path.exists(db_path):
                        conn = sqlite3.connect(db_path)
                        conn.row_factory = sqlite3.Row
                        cursor = conn.cursor()
                        
                        # Look for a file that matches our current pdf_path
                        current_stored_filename = os.path.basename(pdf_path)
                        cursor.execute('SELECT original_filename FROM files WHERE stored_filename = ?', (current_stored_filename,))
                        row = cursor.fetchone()
                        
                        if row:
                            original_filename = row['original_filename'].upper()
                            logger.info(f"Found original filename from database: {original_filename}")
                        
                        conn.close()
                        
                except Exception as e:
                    logger.warning(f"Could not access database for filename lookup: {e}")
            
            # Method 3: Use current path filename
            if not original_filename:
                original_filename = os.path.basename(pdf_path).upper()
                logger.info(f"Using current path for detection: {original_filename}")
            
            # Method 4: Check uploads directory for SBI files (last resort)
            if not original_filename or ('Q1' not in original_filename and 'FY' not in original_filename):
                uploads_dir = "uploads"
                if os.path.exists(uploads_dir):
                    for filename in os.listdir(uploads_dir):
                        if filename.upper().endswith('.PDF') and ('SBI' in filename.upper() or 'HDFC' in filename.upper()):
                            original_filename = filename.upper()
                            logger.info(f"Found SBI/HDFC file in uploads: {filename}")
                            break
            
            # Determine page range based on filename
            if 'Q1' in original_filename:
                logger.info(f"Q1 file detected: {original_filename}. Using pages 3-4 for L-1-A-REVENUE/RA")
                return "3-4"
            elif 'FY' in original_filename:
                logger.info(f"FY file detected: {original_filename}. Using pages 3-6 for L-1-A-REVENUE/RA")
                return "3-6"
            else:
                # Default to Q1 behavior if neither Q1 nor FY is clearly detected
                logger.info(f"Default file detected: {original_filename}. Using pages 3-4 for L-1-A-REVENUE/RA")
                return "3-4"
            
        # Get forms list for other forms
        forms = await list_forms(company)

        # Find matching form
        for form in forms:
            if form["form_no"].upper() == form_no.upper():
                return form["pages"]

        return None

    except Exception as e:
        logger.error(f"Error finding pages for form {form_no}: {e}")
        return None


async def _extract_table_data(pdf_path: str, pages_str: str, headers: List[str]) -> List[Dict[str, Any]]:
    """
    Extract table data from PDF pages using Camelot, focusing on the specific form table.
    Extract all relevant tables from the specified pages with improved precision.
    """
    try:
        if not CAMELOT_AVAILABLE:
            logger.warning("Camelot not available, trying Tabula fallback")
            if TABULA_AVAILABLE:
                rows = _extract_with_tabula(pdf_path, pages_str, headers)
                if rows:
                    return rows
            logger.warning("Tabula fallback not available or returned no rows; returning empty rows")
            return []

        # Parse pages string (e.g., "7-10" or "7")
        if '-' in pages_str:
            start_page, end_page = pages_str.split('-')
            pages_range = f"{start_page}-{end_page}"
        else:
            pages_range = pages_str

        logger.info(f"Extracting tables from pages: {pages_range}")

        # Try both lattice and stream flavors for best results
        all_tables = []

        # Extract tables using Camelot with lattice (best for structured tables)
        try:
            lattice_tables = camelot.read_pdf(
                pdf_path,
                pages=pages_range,
                flavor='lattice',
                strip_text='\n',
                split_text=True,
                line_scale=40  # Better line detection
            )
            if lattice_tables:
                all_tables.extend([(table, 'lattice')
                                  for table in lattice_tables])
                logger.info(f"Found {len(lattice_tables)} tables with lattice")
        except Exception as e:
            logger.warning(f"Lattice extraction failed: {e}")

        # Also try stream flavor for comparison
        try:
            stream_tables = camelot.read_pdf(
                pdf_path,
                pages=pages_range,
                flavor='stream',
                strip_text='\n',
                edge_tol=500
            )
            if stream_tables:
                all_tables.extend([(table, 'stream')
                                  for table in stream_tables])
                logger.info(f"Found {len(stream_tables)} tables with stream")
        except Exception as e:
            logger.warning(f"Stream extraction failed: {e}")

        extracted_rows = []
        tables_processed = 0
        tables_found = len(all_tables)

        logger.info(
            f"Found {tables_found} total tables in pages {pages_range}")

        if all_tables:
            # Process tables in parallel using ThreadPoolExecutor
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                # Submit all table processing tasks with correct parameters
                future_to_index = {}
                for idx, table_data in enumerate(all_tables):
                    future = executor.submit(_process_table_with_threading, (table_data, idx), headers)
                    future_to_index[future] = idx

                # Collect results as they complete
                for future in concurrent.futures.as_completed(future_to_index):
                    table_idx = future_to_index[future]
                    try:
                        # 30 second timeout per table
                        table_rows = future.result(timeout=30)
                        if table_rows:
                            extracted_rows.extend(table_rows)
                            tables_processed += 1
                            logger.info(
                                f"Completed processing table {table_idx + 1} with {len(table_rows)} rows")
                    except Exception as e:
                        logger.error(
                            f"Error processing table {table_idx + 1}: {e}")

        logger.info(
            f"Multi-threaded summary: Found {tables_found} tables, processed {tables_processed} relevant tables, extracted {len(extracted_rows)} total rows")

        if not extracted_rows and TABULA_AVAILABLE:
            logger.info("Camelot returned no rows; attempting Tabula fallback")
            extracted_rows = _extract_with_tabula(pdf_path, pages_range, headers)

        if not extracted_rows:
            logger.warning(
                "No matching tables found for the provided template headers. Returning empty rows.")
            return []

        logger.info(
            f"Final threaded extraction result: {len(extracted_rows)} rows from {tables_processed} tables")
        return extracted_rows

    except Exception as e:
        logger.error(f"Error in threaded table extraction: {e}")
        import traceback
        traceback.print_exc()
        # Return empty data on error
        return []


# Tabula fallback

def _extract_with_tabula(pdf_path: str, pages: str, headers: List[str]) -> List[Dict[str, Any]]:
    try:
        if not TABULA_AVAILABLE:
            return []
        # Tabula expects page ranges like '2-4' or a list
        page_arg = pages
        # Try both lattice and stream
        dfs = []
        try:
            dfs += tabula.read_pdf(pdf_path, pages=page_arg, lattice=True, multiple_tables=True)
        except Exception as e:
            logger.warning(f"Tabula lattice failed: {e}")
        try:
            dfs += tabula.read_pdf(pdf_path, pages=page_arg, stream=True, multiple_tables=True)
        except Exception as e:
            logger.warning(f"Tabula stream failed: {e}")

        rows: List[Dict[str, Any]] = []
        for idx, df in enumerate(dfs or []):
            if df is None or df.empty or df.shape[0] < 2:
                continue
            df = df.fillna('')
            logger.info(f"Processing Tabula table {idx+1}: Shape {df.shape}")
            
            # More aggressive row extraction - start from row 0 and check all rows
            for row_idx in range(len(df)):
                series = df.iloc[row_idx]
                values = [str(v).strip() for v in series.tolist()]
                if not any(v for v in values):
                    continue
                    
                row: Dict[str, Any] = {}
                for i, h in enumerate(headers):
                    row[h] = values[i] if i < len(values) else ''
                
                # More lenient filtering - accept any row with meaningful content
                particulars = (row.get(headers[0]) or '').strip()
                has_content = any((row.get(h) or '').strip() for h in headers)
                
                # Skip obvious header rows and empty rows
                if (particulars and 
                    not particulars.lower() in ['particulars', 'schedule', 'life', 'pension'] and
                    has_content and
                    len(particulars) > 2):
                    rows.append(row)
                    logger.info(f"Added Tabula row: {particulars[:50]}...")
                    
        logger.info(f"Tabula extracted {len(rows)} total rows from {len(dfs)} tables")
        return rows
    except Exception as e:
        logger.error(f"Tabula extraction error: {e}")
        return []


def _create_dummy_data(headers: List[str]) -> List[Dict[str, Any]]:
    """
    Create dummy data when extraction fails
    """
    dummy_rows = []
    for i in range(3):  # Create 3 dummy rows
        row = {}
        for j, header in enumerate(headers):
            row[header] = f"Sample data {i+1}-{j+1}"
        dummy_rows.append(row)
    return dummy_rows


def _convert_dataframe_to_rows(df, template_headers: List[str], table_num: int = 1) -> List[Dict[str, Any]]:
    """
    Fallback conversion function - use the improved L-4 conversion instead
    """
    return _convert_l4_dataframe_to_rows(df, template_headers, table_num, "fallback")


# NEW: Generic table matcher that works for multiple forms using template headers

def _is_table_matching_headers(df, headers: List[str]) -> bool:
    """Return True if a table's top rows look compatible with the template headers.

    Heuristics:
    - Table must have at least as many columns as the template (or within 2 columns slack)
    - The first 2-3 rows should contain header-like keywords (e.g., 'Particulars')
    - Avoid obviously unrelated tables by checking absence of common noise-only patterns
    """
    try:
        if df.empty or df.shape[0] < 2:
            return False

        num_cols = df.shape[1]
        # allow some slack as Camelot may split/merge columns
        if num_cols < max(2, min(len(headers), 6)):
            return False

        # Concatenate first three rows to inspect header text
        head_rows = df.head(min(3, len(df))).astype(str).values.flatten()
        head_text = ' '.join(head_rows).upper()

        # Basic signals that this is a business schedule table
        has_particulars = 'PARTICULARS' in head_text or 'PARTICULAR' in head_text
        has_business_terms = any(term in head_text for term in [
            'LINKED', 'NON-LINKED', 'PARTICIPATING', 'NON-PARTICIPATING', 'REVENUE', 'ACCOUNT', 'SCHEDULE', 'TOTAL'
        ])

        return has_particulars or has_business_terms
    except Exception as e:
        logger.error(f"Error matching table headers: {e}")
        return False


def _process_table_with_threading(table_data_and_index, headers: List[str]) -> List[Dict[str, Any]]:
    """
    Process a single table in a thread-safe manner (fixed parameter issue)
    """
    table_data, table_idx = table_data_and_index
    table, flavor = table_data
    df = table.df

    logger.info(
        f"Thread processing table {table_idx + 1} ({flavor}): Shape {df.shape}")

    # Skip empty tables
    if df.empty or df.shape[0] < 2:
        logger.info(
            f"Thread: Skipping table {table_idx + 1} - empty or too small")
        return []

    # Prefer tables that match headers, but still attempt conversion as fallback
    rows: List[Dict[str, Any]] = []
    try:
        if _is_table_matching_headers(df, headers):
            logger.info(
                f"Thread: Processing table {table_idx + 1} ({flavor}) - matches template structure")
            rows = _convert_l4_dataframe_to_rows(df, headers, table_idx + 1, flavor)
        else:
            logger.info(
                f"Thread: Attempting conversion for table {table_idx + 1} despite header mismatch")
            rows = _convert_l4_dataframe_to_rows(df, headers, table_idx + 1, flavor)
    except Exception as e:
        logger.error(f"Thread: Error converting table {table_idx + 1}: {e}")
        rows = []

    return rows


async def _extract_table_data_threaded(pdf_path: str, pages_str: str, headers: List[str]) -> List[Dict[str, Any]]:
    """
    Extract table data using multi-threading for faster processing (corrected)
    """
    try:
        if not CAMELOT_AVAILABLE:
            logger.warning("Camelot not available, returning dummy data")
            return _create_dummy_data(headers)

        # Parse pages string (e.g., "7-10" or "7")
        if '-' in pages_str:
            start_page, end_page = pages_str.split('-')
            pages_range = f"{start_page}-{end_page}"
        else:
            pages_range = pages_str

        logger.info(
            f"Extracting tables from pages: {pages_range} (with multi-threading)")

        # Try both lattice and stream flavors for best results
        all_tables = []

        # Extract tables using Camelot with lattice (best for structured tables)
        try:
            lattice_tables = camelot.read_pdf(
                pdf_path,
                pages=pages_range,
                flavor='lattice',
                strip_text='\n',
                split_text=True,
                line_scale=40  # Better line detection
            )
            if lattice_tables:
                all_tables.extend([(table, 'lattice')
                                  for table in lattice_tables])
                logger.info(f"Found {len(lattice_tables)} tables with lattice")
        except Exception as e:
            logger.warning(f"Lattice extraction failed: {e}")

        # Also try stream flavor for comparison
        try:
            stream_tables = camelot.read_pdf(
                pdf_path,
                pages=pages_range,
                flavor='stream',
                strip_text='\n',
                edge_tol=500
            )
            if stream_tables:
                all_tables.extend([(table, 'stream')
                                  for table in stream_tables])
                logger.info(f"Found {len(stream_tables)} tables with stream")
        except Exception as e:
            logger.warning(f"Stream extraction failed: {e}")

        extracted_rows = []
        tables_processed = 0
        tables_found = len(all_tables)

        logger.info(
            f"Found {tables_found} total tables in pages {pages_range}")

        if all_tables:
            # Process tables in parallel using ThreadPoolExecutor
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                # Create data with indices
                table_data_with_indices = [(table_data, idx)
                                           for idx, table_data in enumerate(all_tables)]

                # Submit all table processing tasks
                future_to_index = {
                    executor.submit(_process_table_with_threading, data, headers): idx
                    for idx, data in enumerate(table_data_with_indices)
                }

                # Collect results as they complete
                for future in concurrent.futures.as_completed(future_to_index):
                    table_idx = future_to_index[future]
                    try:
                        # 30 second timeout per table
                        table_rows = future.result(timeout=30)
                        if table_rows:
                            extracted_rows.extend(table_rows)
                            tables_processed += 1
                            logger.info(
                                f"Completed processing table {table_idx + 1} with {len(table_rows)} rows")
                    except Exception as e:
                        logger.error(
                            f"Error processing table {table_idx + 1}: {e}")

        logger.info(
            f"Multi-threaded summary: Found {tables_found} tables, processed {tables_processed} relevant tables, extracted {len(extracted_rows)} total rows")

        if not extracted_rows:
            logger.warning(
                "No suitable L-4 Premium tables found, returning dummy data")
            return _create_dummy_data(headers)

        logger.info(
            f"Final threaded extraction result: {len(extracted_rows)} rows from {tables_processed} tables")
        return extracted_rows

    except Exception as e:
        logger.error(f"Error in threaded table extraction: {e}")
        import traceback
        traceback.print_exc()
        # Return dummy data on error
        return _create_dummy_data(headers)


def _flatten_headers(headers) -> List[str]:
    """
    Flatten complex header structure to a simple list of column names
    """
    if isinstance(headers, list):
        # Already a flat list (HDFC format)
        return headers
    elif isinstance(headers, dict):
        # Complex nested structure (SBI format) - flatten to column list
        flat_headers = []
        for main_key, sub_headers in headers.items():
            if isinstance(sub_headers, list):
                for sub_header in sub_headers:
                    if sub_header:  # Skip empty strings
                        flat_headers.append(sub_header)
            elif isinstance(sub_headers, dict):
                # Handle nested dict like "Non-Linked Business"
                for sub_key, sub_sub_headers in sub_headers.items():
                    if isinstance(sub_sub_headers, list):
                        for sub_sub_header in sub_sub_headers:
                            if sub_sub_header:  # Skip empty strings
                                flat_headers.append(sub_sub_header)
            else:
                flat_headers.append(str(sub_headers))
        return flat_headers
    else:
        return [str(headers)]


# Additional utility functions

def get_available_companies() -> List[str]:
    """Get list of companies with uploaded PDFs"""
    companies = []
    if os.path.exists(PDFS_DIR):
        for file in os.listdir(PDFS_DIR):
            if file.lower().endswith('.pdf'):
                company_name = os.path.splitext(file)[0]
                companies.append(company_name)
    return sorted(companies)


def get_company_templates(company: str) -> List[Dict[str, Any]]:
    """Get list of templates for a company"""
    templates = []
    templates_path = os.path.join(TEMPLATES_DIR, company.lower())

    if os.path.exists(templates_path):
        for file in os.listdir(templates_path):
            if file.lower().endswith('.json'):
                try:
                    template_path = os.path.join(templates_path, file)
                    with open(template_path, 'r', encoding='utf-8') as f:
                        template_data = json.load(f)
                        templates.append({
                            "form_no": template_data.get("Form No", ""),
                            "title": template_data.get("Title", ""),
                            "headers": template_data.get("Headers", []),
                            "file": file
                        })
                except Exception as e:
                    logger.warning(f"Error reading template {file}: {e}")

    return templates


def create_sample_templates():
    """Create sample templates for testing"""
    try:
        # Sample SBI templates
        sbi_dir = os.path.join(TEMPLATES_DIR, "sbi")
        os.makedirs(sbi_dir, exist_ok=True)

        # L-4 Premium Schedule template
        l4_template = {
            "Form No": "L-4",
            "Title": "Premium Schedule",
            "Headers": {
                "Particulars": [
                    "Particulars"
                ],
                "Unit Linked Business": [
                    "Life",
                    "Pension",
                    "Total"
                ],
                "Non-Linked Business": {
                    "Participating": [
                        "Life",
                        "Pension",
                        "Var. Ins",
                        "Total"
                    ],
                    "Non-Participating": [
                        "Life",
                        "Annuity",
                        "Pension",
                        "Health",
                        "Var. Ins",
                        "Total"
                    ]
                },
                "Grand Total": [
                    "Grand Total"
                ]
            },
            "Rows": []
        }

        with open(os.path.join(sbi_dir, "l-4.json"), 'w', encoding='utf-8') as f:
            json.dump(l4_template, f, indent=2, ensure_ascii=False)

        # L-5 Commission Schedule template
        l5_template = {
            "Form No": "L-5",
            "Title": "Commission Schedule",
            "Headers": {
                "Particulars": [
                    "Particulars"
                ],
                "Unit Linked Business": [
                    "Life",
                    "Pension",
                    "Total"
                ],
                "Non-Linked Business": {
                    "Participating": [
                        "Life",
                        "Pension",
                        "Var. Ins",
                        "Total"
                    ],
                    "Non-Participating": [
                        "Life",
                        "Annuity",
                        "Pension",
                        "Health",
                        "Var. Ins",
                        "Total"
                    ]
                },
                "Grand Total": [
                    "Grand Total"
                ]
            },
            "Rows": []
        }

        with open(os.path.join(sbi_dir, "l-5.json"), 'w', encoding='utf-8') as f:
            json.dump(l5_template, f, indent=2, ensure_ascii=False)

        logger.info("Sample templates created successfully")

    except Exception as e:
        logger.error(f"Error creating sample templates: {e}")


# Initialize sample templates on module load
if __name__ == "__main__":
    create_sample_templates()


def _extract_l4_premium_clean(pdf_path: str, pages_str: str, template_headers: List[str]) -> List[Dict[str, Any]]:
    """
    Clean extraction specifically for L-4 Premium tables
    """
    try:
        if not CAMELOT_AVAILABLE:
            logger.warning("Camelot not available")
            return []

        logger.info(
            f"Extracting L-4 Premium from {pdf_path}, pages {pages_str}")

        # Extract tables with lattice
        tables = camelot.read_pdf(pdf_path, pages=pages_str, flavor='lattice')
        logger.info(f"Found {len(tables)} tables with lattice")

        all_rows = []

        # Process each table
        for i, table in enumerate(tables):
            df = table.df
            logger.info(f"Processing table {i+1}: Shape {df.shape}")

            # Check if this is an L-4 Premium table
            if not _is_l4_premium_table_clean(df):
                logger.info(
                    f"Skipping table {i+1} - not L-4 Premium structure")
                continue

            logger.info(f"Processing L-4 Premium table {i+1}")

            # Extract rows from this table
            table_rows = _extract_rows_from_l4_table(df, template_headers)
            all_rows.extend(table_rows)
            logger.info(f"Extracted {len(table_rows)} rows from table {i+1}")

        logger.info(
            f"Total extracted: {len(all_rows)} rows from {len(tables)} tables")
        
        # Remove duplicate headers
        all_rows = _remove_duplicate_headers(all_rows, template_headers)
        logger.info(f"After removing duplicate headers: {len(all_rows)} rows")
        
        return all_rows

    except Exception as e:
        logger.error(f"Error in L-4 Premium extraction: {e}")
        import traceback
        traceback.print_exc()
        return []


def _is_l4_premium_table_clean(df) -> bool:
    """
    Check if this DataFrame is an L-4 Premium table
    """
    try:
        # Check shape - should have ~15 columns for L-4 Premium
        if df.shape[1] < 10:
            return False

        # Check content
        table_text = df.astype(str).values.flatten()
        table_content = ' '.join(table_text).upper()

        has_particulars = 'PARTICULARS' in table_content
        has_linked = 'LINKED' in table_content
        has_participating = 'PARTICIPATING' in table_content
        has_premium_terms = any(term in table_content for term in [
                                'PREMIUM', 'FIRST YEAR', 'RENEWAL'])

        # Exclude commission tables for premium extraction
        has_commission = 'COMMISSION' in table_content
        has_operating = any(term in table_content for term in [
                            'EMPLOYEE', 'REMUNERATION', 'TRAVEL', 'TRAINING'])

        # Must have premium characteristics but not commission/operating
        return has_particulars and has_linked and has_participating and has_premium_terms and not has_commission and not has_operating

    except Exception as e:
        logger.error(f"Error checking L-4 table: {e}")
        return False


def validate_var_ins_data(row_dict, template_headers):
    """
    Validate that VAR. INS columns show hyphens when they should be empty/zero
    """
    # Check if this is a 'Premiums earned - net' row
    particulars = row_dict.get('Particulars', '').lower()
    
    # If this is a premiums earned row, VAR. INS should typically be empty/hyphen
    if 'premium' in particulars and 'earned' in particulars:
        # Find VAR. INS columns and set them to hyphen if they have values
        for header in template_headers:
            if 'VAR. INS' in header or 'VAR.INS' in header:
                current_value = row_dict.get(header, '').strip()
                # If there's a numeric value, it might be wrong - set to hyphen
                if current_value and re.search(r'\d', current_value):
                    # Check if this looks like it might be from a different column
                    # If it's a large number, it's probably misaligned
                    if len(current_value.replace(',', '').replace('.', '')) > 3:
                        row_dict[header] = '-'
                        print(f"Fixed misaligned VAR. INS value: {current_value} -> -")
    
    return row_dict

def create_intelligent_column_mapping(df, template_headers):
    """
    Create intelligent column mapping by matching actual column headers to template headers
    """
    # Get the actual column headers from the DataFrame
    actual_headers = [str(col).strip() for col in df.columns.tolist()]
    
    # Create mapping from template header to actual column index
    column_mapping = {}
    
    # Define patterns for matching headers
    header_patterns = {
        'Particulars': [r'particulars?', r'description', r'item'],
        'Schedule': [r'schedule', r'ref', r'reference'],
        'LIFE': [r'life', r'linked.*life', r'non.*linked.*life'],
        'PENSION': [r'pension', r'linked.*pension', r'non.*linked.*pension'],
        'HEALTH': [r'health', r'linked.*health', r'non.*linked.*health'],
        'VAR. INS': [r'var\.?\s*ins', r'variable.*insurance', r'linked.*var', r'non.*linked.*var'],
        'TOTAL': [r'total', r'sub.*total'],
        'ANNUITY': [r'annuity', r'non.*linked.*annuity'],
        'GRAND TOTAL': [r'grand.*total', r'total.*total']
    }
    
    # Map each template header to actual column index
    for i, template_header in enumerate(template_headers):
        template_header_clean = template_header.strip()
        best_match_idx = None
        best_match_score = 0
        
        for j, actual_header in enumerate(actual_headers):
            actual_header_clean = actual_header.strip().lower()
            
            # Direct match
            if template_header_clean.lower() == actual_header_clean:
                best_match_idx = j
                best_match_score = 100
                break
            
            # Pattern matching
            for pattern_name, patterns in header_patterns.items():
                if pattern_name.lower() in template_header_clean.lower():
                    for pattern in patterns:
                        if re.search(pattern, actual_header_clean, re.IGNORECASE):
                            score = len(pattern) / len(actual_header_clean) * 50
                            if score > best_match_score:
                                best_match_idx = j
                                best_match_score = score
                                break
        
        if best_match_idx is not None:
            column_mapping[i] = best_match_idx
        else:
            # Fallback to positional mapping
            column_mapping[i] = i if i < len(actual_headers) else 0
    
    return column_mapping


def _extract_rows_from_l4_table(df, template_headers: List[str]) -> List[Dict[str, Any]]:
    """
    Extract structured rows from an L-4 Premium DataFrame
    """
    try:
        rows = []

        # Clean DataFrame
        df = df.fillna('')
        df = df.astype(str)

        # Data starts from row 3 (0-indexed) based on our analysis
        data_start_row = 3

        for row_idx in range(data_start_row, len(df)):
            row_data = df.iloc[row_idx].tolist()

            # Skip empty rows
            if not any(cell.strip() and cell.strip() != 'nan' for cell in row_data):
                continue

            # Get particulars (first column)
            particulars = str(row_data[0]).strip() if len(row_data) > 0 else ""
            if not particulars or particulars == 'nan' or len(particulars) < 2:
                continue

            # Clean particulars text
            clean_particulars = _clean_particulars_text(particulars)

            # Skip header rows that contain column names
            header_keywords = ["Particulars", "Schedule", "Unit_Linked_Life", "Unit_Linked_Pension", 
                             "Participating_Life", "Non_Participating_Life", "Grand_Total"]
            if any(keyword.lower() in clean_particulars.lower() for keyword in header_keywords):
                logger.info(f"Skipping header row: {clean_particulars[:40]}...")
                continue

            # Build row dictionary
            row_dict = {}

            # Create intelligent column mapping
            column_mapping = create_intelligent_column_mapping(df, template_headers)
            
            # Map to template headers using intelligent mapping
            for i, header in enumerate(template_headers):
                if i == 0:  # Particulars column
                    row_dict[header] = clean_particulars
                else:
                    # Use intelligent mapping to find the correct column
                    mapped_idx = column_mapping.get(i, i)
                    if mapped_idx < len(row_data):
                        # Clean numeric value
                        raw_value = str(row_data[mapped_idx]).strip()
                        clean_value = _clean_numeric_value(raw_value)
                        row_dict[header] = clean_value
                    else:
                        row_dict[header] = ""

            # Validate VAR. INS data
            row_dict = validate_var_ins_data(row_dict, template_headers)
            
            # Only add if has meaningful data
            numeric_count = sum(1 for v in row_dict.values()
                                if v and re.search(r'\d', v))
            if clean_particulars and numeric_count >= 1:
                rows.append(row_dict)
                logger.info(f"Added row: {clean_particulars[:40]}...")

        return rows

    except Exception as e:
        logger.error(f"Error extracting rows: {e}")
        return []


def _clean_particulars_text(text: str) -> str:
    """
    Clean the particulars column text
    """
    if not text or text.strip() == 'nan':
        return ""

    # Remove extra whitespace and newlines
    cleaned = re.sub(r'\s+', ' ', text.strip())

    # Fix common issues
    cleaned = cleaned.replace('\n', ' ')
    cleaned = re.sub(r'\s+', ' ', cleaned)
    
    # Fix hyphen character issues - normalize different types of hyphens to standard hyphen
    # This specifically addresses the VAR. INS hyphen issue
    cleaned = cleaned.replace('–', '-')  # en-dash to hyphen
    cleaned = cleaned.replace('—', '-')  # em-dash to hyphen
    cleaned = cleaned.replace('‐', '-')  # hyphen-minus to hyphen
    cleaned = cleaned.replace('‑', '-')  # non-breaking hyphen to hyphen
    
    # Ensure VAR. INS has proper hyphen formatting
    cleaned = re.sub(r'VAR\.\s*INS', 'VAR. INS', cleaned)
    
    return cleaned.strip()


def _clean_numeric_value(value: str) -> str:
    """
    Clean numeric values, properly separating concatenated numbers
    """
    if not value or value.strip() == 'nan':
        return ""

    # Remove whitespace and newlines
    cleaned = value.replace('\n', ' ').strip()

    # If no digits, return as is
    if not re.search(r'\d', cleaned):
        return ""

    # Handle concatenated numbers with line breaks
    # Example: "2,12,587\n        \n5,18,485\n           \n60,044"
    # Should become: "2,12,587" (take the first number)

    # Split on newlines and whitespace, find the first valid number
    parts = re.split(r'[\n\s]+', cleaned)

    for part in parts:
        part = part.strip()
        if part and re.search(r'\d', part):
            # Clean this part
            clean_part = re.sub(r'[^\d,\-\(\)]', '', part)
            if clean_part and len(clean_part.replace(',', '')) >= 1:
                return clean_part

    # Fallback: just clean and return first reasonable number
    cleaned = re.sub(r'[^\d,\-\(\)\s]', '', cleaned)
    cleaned = re.sub(r'\s+', '', cleaned)

    return cleaned if cleaned else ""


def _extract_clean_form_data(pdf_path: str, pages_str: str, template_headers: List[str], form_type: str) -> List[Dict[str, Any]]:
    """
    Clean extraction for all supported forms using form-specific logic
    """
    try:
        if not CAMELOT_AVAILABLE:
            logger.warning("Camelot not available")
            return []

        logger.info(
            f"Extracting {form_type} from {pdf_path}, pages {pages_str}")

        # Extract tables with lattice
        tables = camelot.read_pdf(pdf_path, pages=pages_str, flavor='lattice')
        logger.info(f"Found {len(tables)} tables with lattice")

        all_rows = []

        # Process each table
        for i, table in enumerate(tables):
            df = table.df
            logger.info(f"Processing table {i+1}: Shape {df.shape}")

            # Check if this is a relevant table for the specific form type
            is_relevant = False
            if form_type == "L-4-PREMIUM":
                is_relevant = _is_l4_premium_table_clean(df)
            elif form_type == "L-5-COMMISSION":
                is_relevant = _is_l5_commission_table(df)
            elif form_type == "L-6-OPERATING":
                is_relevant = _is_l6_operating_table(df)

            if not is_relevant:
                logger.info(
                    f"Skipping table {i+1} - not {form_type} structure")
                continue

            logger.info(f"Processing {form_type} table {i+1}")

            # Extract rows from this table
            table_rows = _extract_rows_from_l4_table(df, template_headers)
            all_rows.extend(table_rows)
            logger.info(f"Extracted {len(table_rows)} rows from table {i+1}")

        logger.info(
            f"Total extracted: {len(all_rows)} rows from {len(tables)} tables")
        return all_rows

    except Exception as e:
        logger.error(f"Error in {form_type} extraction: {e}")
        return []


def _is_l5_commission_table(df) -> bool:
    """
    Check if this DataFrame is an L-5 Commission table
    """
    try:
        if df.shape[1] < 10:
            return False

        table_text = df.astype(str).values.flatten()
        table_content = ' '.join(table_text).upper()

        has_particulars = 'PARTICULARS' in table_content
        has_linked = 'LINKED' in table_content
        has_participating = 'PARTICIPATING' in table_content
        has_commission = 'COMMISSION' in table_content

        # Should have commission but not operating expenses
        has_operating = any(term in table_content for term in [
                            'EMPLOYEE', 'REMUNERATION', 'TRAVEL', 'TRAINING'])

        return has_particulars and has_linked and has_participating and has_commission and not has_operating

    except Exception as e:
        logger.error(f"Error checking L-5 table: {e}")
        return False


def _is_l6_operating_table(df) -> bool:
    """
    Check if this DataFrame is an L-6 Operating table
    """
    try:
        if df.shape[1] < 10:
            return False

        table_text = df.astype(str).values.flatten()
        table_content = ' '.join(table_text).upper()

        has_particulars = 'PARTICULARS' in table_content
        has_linked = 'LINKED' in table_content
        has_participating = 'PARTICIPATING' in table_content
        has_operating = any(term in table_content for term in [
                            'EMPLOYEE', 'REMUNERATION', 'TRAVEL', 'TRAINING', 'WELFARE'])

        # Should have operating expenses but not commission
        has_commission = 'COMMISSION' in table_content

        return has_particulars and has_linked and has_participating and has_operating and not has_commission

    except Exception as e:
        logger.error(f"Error checking L-6 table: {e}")
        return False


def _convert_l4_dataframe_to_rows(df, template_headers: List[str], table_num: int, flavor: str) -> List[Dict[str, Any]]:
    """
    Convert L-4 DataFrame to structured rows
    """
    try:
        logger.info(
            f"Converting L-4 table {table_num} ({flavor}) with {len(template_headers)} headers")

        # Use the existing extraction logic
        return _extract_rows_from_l4_table(df, template_headers)

    except Exception as e:
        logger.error(f"Error converting L-4 dataframe: {e}")
        return []


def _is_l4_premium_table(df, headers: List[str]) -> bool:
    """
    Check if this DataFrame is an L-4 Premium table (original function)
    """
    try:
        # Use the cleaner version
        return _is_l4_premium_table_clean(df)
    except Exception as e:
        logger.error(f"Error checking L-4 table: {e}")
        return False


def _find_specific_form_pages(pdf_path: str, form_no: str) -> Optional[str]:
    """
    Find specific pages for a form by searching PDF content
    """
    try:
        doc = fitz.open(pdf_path)
        found_pages = []

        # Search for form number patterns
        search_terms = [
            f"FORM {form_no}",
            f"Form {form_no}",
            form_no.upper(),
            form_no.replace('-', ' '),
        ]

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text = page.get_text().upper()

            for term in search_terms:
                if term.upper() in text:
                    found_pages.append(page_num + 1)
                    break

        doc.close()

        if found_pages:
            if len(found_pages) == 1:
                return str(found_pages[0])
            else:
                # Return a range limited to reasonable size
                start = min(found_pages)
                end = min(max(found_pages), start + 5)  # Limit to 6 pages max
                return f"{start}-{end}"

        return None

    except Exception as e:
        logger.error(f"Error finding specific pages for {form_no}: {e}")
        return None


async def ai_extract_form(company: str, form_no: str) -> List[Dict[str, Any]]:
    """
    🤖 AI PDF Form Extractor - Complete Implementation

    Follows the exact workflow:
    1. Load template JSON from templates/{company}/{form_no}.json
    2. Parse PDF and find all periods/instances of the form
    3. Extract data using Camelot for tables + text extraction for fields
    4. Return structured JSON for ALL periods found

    Returns List of extracted form data, one per period/year found.
    """
    try:
        pdf_path = os.path.join(PDFS_DIR, f"{company}.pdf")
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF not found for company: {company}")

        # Step 1: Load Template JSON
        template_path = await _find_template_for_form(company, form_no)
        with open(template_path, 'r', encoding='utf-8') as f:
            template = json.load(f)

        logger.info(
            f"🤖 AI Extractor loaded template: {template.get('Title')} for {form_no}")

        # Step 2: Find ALL instances of this form in PDF (multiple periods)
        form_instances = await _find_all_form_instances(pdf_path, form_no, template)

        if not form_instances:
            logger.warning(
                f"No instances found for {form_no}, creating default extraction")
            form_instances = [{"pages": "1-5", "period": "Default Period"}]

        logger.info(f"🤖 Found {len(form_instances)} instances of {form_no}")

        # Step 3: Extract data for each instance/period
        extracted_periods = []

        for i, instance in enumerate(form_instances):
            logger.info(f"🤖 Processing instance {i+1}: {instance}")

            # Extract data for this specific instance
            period_data = await _extract_single_period_data(
                pdf_path, instance, template, form_no
            )

            extracted_periods.append(period_data)

        logger.info(
            f"🤖 AI Extraction complete: {len(extracted_periods)} periods extracted")

        return extracted_periods

    except Exception as e:
        logger.error(f"🤖 AI Extraction failed for {form_no} ({company}): {e}")
        raise


async def _find_template_for_form(company: str, form_no: str) -> str:
    """Find the best matching template file for a form."""
    company_templates_dir = os.path.join(TEMPLATES_DIR, company.lower())

    if not os.path.exists(company_templates_dir):
        raise FileNotFoundError(
            f"No templates directory found for company: {company}")

    # Get all template files
    template_files = [f for f in os.listdir(
        company_templates_dir) if f.endswith('.json')]

    # Try exact matches first
    form_clean = form_no.upper().strip()

    for template_file in template_files:
        template_name = template_file.replace('.json', '').upper()

        # Direct match
        if template_name == form_clean:
            return os.path.join(company_templates_dir, template_file)

        # Partial match (L-4 matches L-4-PREMIUM)
        if form_clean in template_name or template_name.startswith(form_clean):
            return os.path.join(company_templates_dir, template_file)

    raise FileNotFoundError(
        f"No template found for {form_no} in {company}. Available: {template_files}")


async def _find_all_form_instances(pdf_path: str, form_no: str, template: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Find ALL instances of a form in the PDF (different periods/years).
    Returns list of {"pages": "7-8", "period": "Dec 2023", "title": "...}
    """
    instances = []

    try:
        doc = fitz.open(pdf_path)

        # Search for form title patterns throughout the document
        form_title = template.get("Title", form_no)

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            page_text = page.get_text()

            # Look for form indicators
            if (_is_form_page(page_text, form_no, form_title)):
                # Extract period information from this page
                period = _extract_period_from_text(page_text)

                # Determine page range (current page + next few pages for table data)
                # Form + 2 more pages typically
                end_page = min(page_num + 3, doc.page_count)
                pages_str = f"{page_num + 1}" if page_num + \
                    1 == end_page else f"{page_num + 1}-{end_page}"

                instance = {
                    "pages": pages_str,
                    "period": period or f"Period on page {page_num + 1}",
                    "start_page": page_num + 1,
                    "title": form_title
                }

                instances.append(instance)
                logger.info(f"🤖 Found form instance: {instance}")

        doc.close()

        # Remove duplicates (same period found on consecutive pages)
        unique_instances = []
        seen_periods = set()

        for instance in instances:
            period_key = instance["period"].strip().lower()
            if period_key not in seen_periods:
                unique_instances.append(instance)
                seen_periods.add(period_key)

        return unique_instances

    except Exception as e:
        logger.error(f"Error finding form instances: {e}")
        return []


def _is_form_page(page_text: str, form_no: str, form_title: str) -> bool:
    """Check if a page contains the specified form."""
    text_upper = page_text.upper()

    # Look for form number patterns
    form_patterns = [
        form_no.upper(),
        f"FORM {form_no.upper()}",
        f"SCHEDULE {form_no.upper()}",
        form_title.upper() if form_title else ""
    ]

    for pattern in form_patterns:
        if pattern and pattern in text_upper:
            return True

    return False


async def _extract_single_period_data(pdf_path: str, instance: Dict[str, Any], template: Dict[str, Any], form_no: str) -> Dict[str, Any]:
    """
    Extract data for a single period/instance of the form.
    Returns complete template structure with extracted data.
    """
    try:
        # Create result structure based on template
        result = {
            "Form": form_no,
            "Title": template.get("Title", ""),
            "Period": instance["period"],
            "PagesUsed": instance["start_page"],
            "Currency": template.get("Currency", "Rs. in Lakhs"),
            "Headers": [],
            "Rows": []
        }

        # Flatten headers for extraction
        template_headers = template.get("Headers", {})
        flat_headers = _flatten_headers(template_headers)
        result["Headers"] = flat_headers

        # Extract table data using Camelot
        if CAMELOT_AVAILABLE:
            extracted_rows = await _extract_table_data(pdf_path, instance["pages"], flat_headers)
        else:
            logger.warning(
                "🤖 Camelot not available, using text extraction fallback")
            extracted_rows = await _extract_text_based_data(pdf_path, instance["pages"], flat_headers)

        result["Rows"] = extracted_rows

        # Extract additional fields from text if needed
        result = await _enhance_with_text_fields(pdf_path, instance["pages"], result)

        logger.info(
            f"🤖 Extracted {len(extracted_rows)} rows for period: {instance['period']}")

        return result

    except Exception as e:
        logger.error(f"Error extracting single period data: {e}")
        # Return template structure with error info
        return {
            "Form": form_no,
            "Title": template.get("Title", ""),
            "Period": f"Extraction Error: {instance.get('period', 'Unknown')}",
            "PagesUsed": instance.get("start_page", 0),
            "Currency": template.get("Currency", "Rs. in Lakhs"),
            "Headers": _flatten_headers(template.get("Headers", {})),
            "Rows": [],
            "Error": str(e)
        }


async def _extract_text_based_data(pdf_path: str, pages_str: str, headers: List[str]) -> List[Dict[str, Any]]:
    """
    Fallback text-based extraction when Camelot is not available.
    """
    try:
        doc = fitz.open(pdf_path)

        # Parse page range
        if '-' in pages_str:
            start_page, end_page = map(int, pages_str.split('-'))
            pages = list(range(start_page, end_page + 1))
        else:
            pages = [int(pages_str)]

        # Extract text from all pages
        all_text = ""
        for page_num in pages:
            if page_num <= doc.page_count:
                page = doc.load_page(page_num - 1)
                all_text += page.get_text() + "\n"

        doc.close()

        # Parse text into rows (basic implementation)
        rows = []
        lines = all_text.split('\n')

        for line in lines:
            line = line.strip()
            if line and not line.isupper() and len(line.split()) >= 3:
                # Try to parse as table row
                parts = line.split()
                if len(parts) >= len(headers):
                    row = {}
                    for i, header in enumerate(headers):
                        if i < len(parts):
                            row[header] = parts[i]
                        else:
                            row[header] = ""
                    rows.append(row)

        return rows[:10]  # Limit to reasonable number for demo

    except Exception as e:
        logger.error(f"Text-based extraction failed: {e}")
        return _create_dummy_data(headers)


async def _enhance_with_text_fields(pdf_path: str, pages_str: str, result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Enhance extracted data with additional text fields like period, currency, etc.
    """
    try:
        doc = fitz.open(pdf_path)

        # Parse page range
        if '-' in pages_str:
            start_page, end_page = map(int, pages_str.split('-'))
            page_num = start_page
        else:
            page_num = int(pages_str)

        if page_num <= doc.page_count:
            page = doc.load_page(page_num - 1)
            page_text = page.get_text()

            # Extract more detailed period if possible
            detailed_period = _extract_period_from_text(page_text)
            if detailed_period and detailed_period != result.get("Period"):
                result["Period"] = detailed_period

            # Extract currency information
            currency_match = re.search(
                r'(Rs\.?\s*in\s*\w+)', page_text, re.IGNORECASE)
            if currency_match:
                result["Currency"] = currency_match.group(1)

        doc.close()

    except Exception as e:
        logger.error(f"Error enhancing with text fields: {e}")

    return result

def _remove_duplicate_headers(rows: List[Dict[str, Any]], headers: List[str]) -> List[Dict[str, Any]]:
    """
    Remove duplicate header rows to fix the duplicate header issue
    """
    try:
        if not rows:
            return rows
            
        # Define header keywords to identify header rows
        header_keywords = [
            "Particulars", "Schedule", "Unit_Linked_Life", "Unit_Linked_Pension", 
            "Unit_Linked_Total", "Participating_Life", "Participating_Pension", 
            "Participating_Var_Ins", "Participating_Total", "Non_Participating_Life", 
            "Non_Participating_Annuity", "Non_Participating_Pension", 
            "Non_Participating_Health", "Non_Participating_Var_Ins", 
            "Non_Participating_Total", "Grand_Total"
        ]
        
        # Filter out header rows
        filtered_rows = []
        header_rows_removed = 0
        
        for row in rows:
            particulars = str(row.get(headers[0] if headers else "Particulars", "")).strip()
            
            # Check if this row contains header keywords
            is_header_row = False
            for keyword in header_keywords:
                if keyword.lower() in particulars.lower():
                    is_header_row = True
                    break
            
            # Also check if the row is too short (likely a header)
            if len(particulars) < 3:
                is_header_row = True
            
            # Skip header rows
            if is_header_row:
                header_rows_removed += 1
                logger.info(f"Removing header row: {particulars[:50]}...")
                continue
            
            # Keep data rows
            filtered_rows.append(row)
        
        logger.info(f"Removed {header_rows_removed} header rows, kept {len(filtered_rows)} data rows")
        return filtered_rows
        
    except Exception as e:
        logger.error(f"Error removing duplicate headers: {e}")
        return rows


def _extract_period_from_text(text: str) -> Optional[str]:
    """
    Extract period information from text content.
    Looks for patterns like "For the quarter ended...", "As at...", etc.
    """
    try:
        if not text:
            return None
            
        text_upper = text.upper()
        
        # Common period patterns
        period_patterns = [
            r'FOR THE QUARTER ENDED\s+([^,\n]+)',
            r'FOR THE PERIOD ENDED\s+([^,\n]+)',
            r'AS AT\s+([^,\n]+)',
            r'FOR THE YEAR ENDED\s+([^,\n]+)',
            r'FOR THE HALF YEAR ENDED\s+([^,\n]+)',
            r'QUARTER ENDED\s+([^,\n]+)',
            r'PERIOD ENDED\s+([^,\n]+)',
            r'YEAR ENDED\s+([^,\n]+)',
            r'HALF YEAR ENDED\s+([^,\n]+)',
        ]
        
        for pattern in period_patterns:
            match = re.search(pattern, text_upper)
            if match:
                period = match.group(1).strip()
                # Clean up the period text
                period = re.sub(r'\s+', ' ', period)
                period = period.replace('\n', ' ')
                return period
                
        return None
        
    except Exception as e:
        logger.error(f"Error extracting period from text: {e}")
        return None
