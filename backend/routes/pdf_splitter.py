from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, Form, Body
from fastapi.responses import FileResponse
from typing import List, Dict, Optional
from pydantic import BaseModel
import os
import json
import re
import sys
import uuid
import subprocess
import traceback
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from services.pdf_splitter import PDFSplitterService


# Load environment variables
load_dotenv()

router = APIRouter(tags=["PDF Splitter"])

# Initialize the PDF splitter service
pdf_splitter = PDFSplitterService()

# Form preferences storage (using JSON file for simplicity)
# Use absolute path in backend directory
BASE_DIR = Path(__file__).parent.parent
FORM_PREFERENCES_FILE = BASE_DIR / "form_preferences.json"

# Pydantic models for form preferences


class FormPreferencesRequest(BaseModel):
    enabled_forms: List[str]


def load_form_preferences() -> Dict:
    """Load form preferences from JSON file"""
    if FORM_PREFERENCES_FILE.exists():
        try:
            with open(FORM_PREFERENCES_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading form preferences: {e}")
            return {}
    return {}


def save_form_preferences(prefs: Dict):
    """Save form preferences to JSON file"""
    try:
        with open(FORM_PREFERENCES_FILE, 'w') as f:
            json.dump(prefs, f, indent=2)
    except Exception as e:
        print(f"Error saving form preferences: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to save preferences: {str(e)}")


@router.post("/upload-and-split")
async def upload_and_split_pdf(
    company_name: str = Form(...),
    user_id: str = Form(...),
    pdf_file: UploadFile = File(...)
):
    """
    Upload a PDF and split it according to index extraction
    """
    try:
        # Validate file type
        if not pdf_file.filename.lower().endswith('.pdf'):
            raise HTTPException(
                status_code=400, detail="Only PDF files are allowed")

        result = pdf_splitter.upload_and_split_pdf(
            company_name, pdf_file, user_id)

        if not result["success"]:
            raise HTTPException(status_code=500, detail=result["error"])

        return {
            "success": True,
            "message": f"PDF uploaded and split into {result['total_splits']} files",
            "data": {
                # "upload_id": result["upload_id"],
                "company_name": result["company_name"],
                "pdf_name": result["pdf_name"],
                "total_splits": result["total_splits"]
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@router.get("/companies/{company_name}/pdfs")
async def get_company_pdfs(company_name: str):
    """
    Get all PDFs for a specific company
    """
    try:
        pdfs = pdf_splitter.get_company_pdfs(company_name)
        return {
            "success": True,
            "company_name": company_name,
            "pdfs": pdfs
        }

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to get PDFs: {str(e)}")


@router.get("/companies/{company_name}/pdfs/{pdf_name}/splits")
async def get_pdf_splits(company_name: str, pdf_name: str):
    """
    Get all split files for a specific PDF
    """
    try:
        splits = pdf_splitter.get_pdf_splits(company_name, pdf_name)
        return {
            "success": True,
            "company_name": company_name,
            "pdf_name": pdf_name,
            "splits": splits,
            "total_splits": len(splits)
        }

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to get splits: {str(e)}")


@router.get("/companies/{company_name}/pdfs/{pdf_name}/splits/{split_filename}/download")
async def download_split_file(company_name: str, pdf_name: str, split_filename: str):
    """
    Download a specific split file
    """
    try:
        file_path = pdf_splitter.get_split_file_path(
            company_name, pdf_name, split_filename)

        if not file_path or not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="Split file not found")

        return FileResponse(
            path=file_path,
            filename=split_filename,
            media_type='application/pdf'
        )

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Download failed: {str(e)}")


@router.get("/companies")
async def get_available_companies():
    """
    Get list of all companies that have PDFs
    """
    try:
        companies = []
        splits_dir = Path("pdf_splits")

        if splits_dir.exists():
            for company_folder in splits_dir.iterdir():
                if company_folder.is_dir():
                    # Convert folder name back to display name
                    company_name = company_folder.name.replace(
                        "_", " ").title()

                    # Count PDFs in this company folder
                    pdf_count = len(
                        [f for f in company_folder.iterdir() if f.is_dir()])

                    if pdf_count > 0:
                        companies.append({
                            "name": company_name,
                            "folder_name": company_folder.name,
                            "pdf_count": pdf_count
                        })

        return {
            "success": True,
            "companies": companies
        }

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to get companies: {str(e)}")


@router.delete("/companies/{company_name}/pdfs/{pdf_name}")
async def delete_pdf_and_splits(company_name: str, pdf_name: str):
    """
    Delete a PDF and all its splits
    """
    try:
        import shutil

        # Delete splits folder
        splits_folder = Path("pdf_splits") / \
            company_name.lower().replace(" ", "_") / pdf_name
        if splits_folder.exists():
            shutil.rmtree(splits_folder)

        # Delete original PDF
        uploads_folder = Path("uploads") / \
            company_name.lower().replace(" ", "_")
        for pdf_file in uploads_folder.glob(f"{pdf_name}.*"):
            if pdf_file.is_file():
                pdf_file.unlink()

        return {
            "success": True,
            "message": f"PDF '{pdf_name}' and all splits deleted successfully"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Delete failed: {str(e)}")


@router.post("/extract-form")
async def extract_form_data(
    company_name: str = Form(...),
    pdf_name: str = Form(...),
    split_filename: str = Form(...),
    user_id: str = Form(...)
):
    """
    Extract form data from a split PDF and correct it with Gemini
    """
    print(f"üéØ EXTRACT FORM FUNCTION CALLED!")

    try:

        print(
            f"üîç Extract form request: company={company_name}, pdf={pdf_name}, split={split_filename}, user={user_id}")

        import subprocess
        import sys
        import uuid
        from datetime import datetime

        # Get the split file path
        split_path = pdf_splitter.get_split_file_path(
            company_name, pdf_name, split_filename)
        if not split_path:
            raise HTTPException(status_code=404, detail="Split file not found")

        # Get split metadata to find form type and template
        splits = pdf_splitter.get_pdf_splits(company_name, pdf_name)
        split_info = next(
            (s for s in splits if s["filename"] == split_filename), None)
        if not split_info:
            raise HTTPException(
                status_code=404, detail="Split metadata not found")

        # Determine template based on form code - IMPROVED DYNAMIC DETECTION
        # FORCE FILENAME EXTRACTION: Always extract from filename to avoid incorrect stored metadata
        # The stored form_code in metadata is often wrong (e.g., L-1 instead of L-1-A)
        # form_code = split_info.get("form_code", "").upper()
        form_code = ""  # Force filename extraction

        # Extract form code from filename (this is more reliable than stored metadata)
        if not form_code:
            # FIXED: Improved form code extraction prioritizing L-X-Y pattern over L-X-Y-Z
            # Order matters - but we prioritize L-X-Y pattern to avoid L-X-Y-Z extraction for RA cases
            patterns = [
                # L-1-A, L-2-A (captures L-X-Y, ignores -RA suffixes)
                r'(L-\d+-[A-Z]+)(?:-[A-Z]+)*',
                # L-6A, L-9A, L-14A (letter suffix without hyphen)
                r'(L-\d+[A-Z]+)',
                # L-10, L-11, L-28 (just numbers) - LAST
                r'(L-\d+)',
            ]

            print(
                f"üîç DEBUG: Testing patterns for filename: {split_filename.upper()}")
            for i, pattern in enumerate(patterns):
                filename_match = re.search(pattern, split_filename.upper())
                print(f"üîç Pattern {i+1}: {pattern}")
                if filename_match:
                    form_code = filename_match.group(1)
                    print(f"  ‚úÖ MATCH: {form_code}")
                    # Normalize underscores to hyphens for consistency
                    form_code = form_code.replace('_', '-')
                    break
                else:
                    print(f"  ‚ùå No match")
            print(f"üîç Final form_code: {form_code}")

        print(
            f"üéØ Form code detected: '{form_code}' from filename: '{split_filename}'")
        print(
            f"üîç Original split info form_code: '{split_info.get('form_code', 'NOT_FOUND')}'")

        # ENHANCED DYNAMIC TEMPLATE SELECTION LOGIC (fully dynamic, no hardcoded company fallbacks)
        templates_root = Path("templates")

        def resolve_company_template_dir(raw_company: str) -> Path:
            """Resolve company dir by tolerant matching (no hardcoded specific fallback)."""
            lower_name = raw_company.lower().strip()
            # try exact
            direct = templates_root / lower_name
            if direct.exists():
                return direct
            # normalize and fuzzy match
            target_norm = re.sub(r"[^a-z0-9]", "", lower_name)
            candidates = []
            for d in templates_root.iterdir():
                if d.is_dir():
                    d_norm = re.sub(r"[^a-z0-9]", "", d.name.lower())
                    if target_norm == d_norm or target_norm.startswith(d_norm) or d_norm.startswith(target_norm):
                        candidates.append(d)
            if candidates:
                return sorted(candidates, key=lambda p: len(p.name))[0]
            return direct  # non-existing path

        def build_template_index() -> Dict[str, List[Dict[str, str]]]:
            """Scan ALL company template folders and build index by base form code.
            Returns: {
                'L-6A': [ { 'company': 'hdfc', 'file': 'L-6A SHAREHOLDERS EXPENSES SCHEDULE.json', 'path': '/abs/path' }, ... ] }
            """
            idx: Dict[str, List[Dict[str, str]]] = {}
            if not templates_root.exists():
                return idx
            for company_dir in templates_root.iterdir():
                if not company_dir.is_dir():
                    continue
                for tf in company_dir.glob("*.json"):
                    stem = tf.stem.upper().replace('_', '-')
                    # Enhanced regex to capture L-6A, L-6, L-1-A etc. correctly
                    # Priority: L-6A over L-6, L-1-A over L-1
                    patterns = [
                        # L-6A, L-9A, L-14A (letters after numbers)
                        r'(L-\d+[A-Z]+)',
                        r'(L-\d+-[A-Z]+)',  # L-1-A, L-2-A (dash then letters)
                        r'(L-\d+)',  # L-6, L-7, L-28 (just numbers)
                        r'(L-\d+" "+[A-Z]+)',  # L-1 A (space then letters)
                        # L-1 AA (space then letters)
                        r'(L-\d+" "+[A-Z][A-Z]+)',
                    ]

                    base = None
                    for pattern in patterns:
                        m = re.search(pattern, stem)
                        if m:
                            base = m.group(1)
                            break

                    if not base:
                        continue

                    # Don't trim for exact matching - keep full form code
                    entry = {"company": company_dir.name,
                             "file": tf.name, "path": str(tf)}
                    idx.setdefault(base, []).append(entry)
                    print(
                        f"Template indexed: {base} -> {tf.name} ({company_dir.name})")
            return idx

        def find_best_template(form_code: str, preferred_company: str, index_map: Dict[str, List[Dict[str, str]]]
                               ):
            if not form_code:
                return None
            form_code_u = form_code.upper().replace('_', '-')

            # Build candidates with priority: exact -> progressive shortening
            candidates_order = []

            # First try exact form code
            candidates_order.append(form_code_u)

            # Then try progressive shortening but be smarter about L-6A vs L-6
            tokens = form_code_u.split('-')

            # For forms like L-6A-SHAREHOLDERS, try L-6A before L-6
            if len(tokens) >= 2 and re.match(r'L-\d+[A-Z]', '-'.join(tokens[:2])):
                # This is like L-6A-SHAREHOLDERS -> try L-6A
                base_with_letter = '-'.join(tokens[:2])  # L-6A
                if base_with_letter not in candidates_order:
                    candidates_order.append(base_with_letter)

            # Then try just the number part L-6
            if len(tokens) >= 2:
                base_number = '-'.join(tokens[:2])  # L-6 from L-6A
                if re.match(r'L-\d+[A-Z]', base_number):
                    # Extract just the number part: L-6A -> L-6
                    # Remove trailing letters
                    number_part = re.sub(r'([A-Z]+)$', '', base_number)
                    if number_part and number_part not in candidates_order:
                        candidates_order.append(number_part)
                elif base_number not in candidates_order:
                    candidates_order.append(base_number)

            # Continue with other progressive shortening
            remaining_tokens = tokens[:]
            while len(remaining_tokens) >= 2:
                remaining_tokens = remaining_tokens[:-1]  # Remove last token
                cand = '-'.join(remaining_tokens)
                if cand not in candidates_order:
                    candidates_order.append(cand)

            # ENHANCED: Try expanded form patterns for common abbreviations
            # This helps match L-5-C with L-5-COMMISSION, L-6-OP with L-6-OPERATING, etc.
            if len(tokens) >= 3:  # Forms like L-5-C, L-6-OP
                base_part = '-'.join(tokens[:2])  # L-5
                abbrev = tokens[2]  # C, OP, etc.

                # Try common expansions
                common_expansions = {
                    'C': ['COMMISSION', 'CLAIMS', 'CURRENT', 'CASH'],
                    'OP': ['OPERATING', 'OPERATIONS'],
                    'EX': ['EXPENSES', 'EXPENDITURE'],
                    'INV': ['INVESTMENT', 'INVESTMENTS'],
                    'SH': ['SHAREHOLDERS', 'SHARE'],
                    'POL': ['POLICYHOLDERS', 'POLICY'],
                    'BEN': ['BENEFITS', 'BENEFICIARY'],
                    'RES': ['RESERVES', 'REVENUE'],
                    'SUP': ['SURPLUS', 'SUPPLEMENTARY'],
                    'LIA': ['LIABILITIES', 'LIABILITY'],
                    'ASS': ['ASSETS', 'ASSESSMENT']
                }

                if abbrev in common_expansions:
                    for expansion in common_expansions[abbrev]:
                        expanded_form = f"{base_part}-{expansion}"
                        if expanded_form not in candidates_order:
                            candidates_order.append(expanded_form)

            print(
                f"Template search candidates for '{form_code}': {candidates_order}")
            preferred_company_l = preferred_company.lower()

            # STRICT COMPANY MATCHING: Only look for templates in the preferred company
            for cand in candidates_order:
                if cand in index_map:
                    for e in index_map[cand]:
                        if e['company'].lower() == preferred_company_l:
                            print(
                                f"EXACT MATCH in preferred company {preferred_company}: {cand} -> {e['file']}")
                            return e

            # NO FALLBACK TO OTHER COMPANIES - force company-specific templates only
            print(
                f"ERROR: No template match found for '{form_code}' in {preferred_company} templates")
            return None

        company_templates_dir = resolve_company_template_dir(company_name)
        template_index = build_template_index()
        template_entry = None

        print(f"Available templates for {company_name} in index:")
        for form_code_key, entries in template_index.items():
            company_entries = [e for e in entries if e['company'].lower(
            ) == company_templates_dir.name.lower()]
            if company_entries:
                print(
                    f"  {form_code_key}: {[e['file'] for e in company_entries]}")

        if form_code:
            template_entry = find_best_template(
                form_code, company_templates_dir.name, template_index)

        if not template_entry:
            # If no template found, list what's available for debugging
            available_forms = []
            print(
                f"No template found for '{form_code}'. Available in {company_templates_dir.name}:")
            for form_code_key, entries in template_index.items():
                company_entries = [e for e in entries if e['company'].lower(
                ) == company_templates_dir.name.lower()]
                if company_entries:
                    available_forms.append(form_code_key)
                    print(f"  {form_code_key}: {company_entries[0]['file']}")

            error_message = f"No template found for form_code '{form_code}' in {company_name} templates."
            if available_forms:
                error_message += f" Available form codes for {company_name}: {', '.join(available_forms)}"
            else:
                error_message += f" No templates found for {company_name}."

            raise HTTPException(status_code=404, detail=error_message)

        template_path = Path(template_entry['path'])
        template_name = template_entry['file']

        # === ENHANCED FILE PATHS DEBUGGING ===
        print(f"\nüîç === FILE PATHS SELECTION DEBUG ===")
        print(f"üìÇ Company Name Input: '{company_name}'")
        print(f"üìÑ PDF Name Input: '{pdf_name}'")
        print(f"üìã Split Filename Input: '{split_filename}'")
        print(f"üéØ Form Code Detected: '{form_code}'")
        print(f"üìÅ Templates Root Directory: {templates_root}")
        print(f"üìÅ Company Templates Directory: {company_templates_dir}")
        print(
            f"üìÅ Company Templates Dir Exists: {company_templates_dir.exists()}")
        print(f"üìÑ Split PDF Path: {split_path}")
        print(f"üìÑ Split PDF Exists: {Path(split_path).exists()}")
        print(
            f"üìÑ Split PDF Size: {Path(split_path).stat().st_size if Path(split_path).exists() else 'N/A'} bytes")
        print(f"üìã Template Selected: {template_name}")
        print(f"üéØ Full Template Path: {template_path}")
        print(f"üìä Template Exists: {template_path.exists()}")
        print(
            f"üìä Template Size: {template_path.stat().st_size if template_path.exists() else 'N/A'} bytes")

        # Create output directories first
        extractions_dir = Path("extractions") / \
            company_name.lower().replace(" ", "_") / pdf_name
        extractions_dir.mkdir(parents=True, exist_ok=True)

        gemini_dir = Path("gemini_verified_json") / \
            company_name.lower().replace(" ", "_") / pdf_name
        gemini_dir.mkdir(parents=True, exist_ok=True)

        extracted_json = extractions_dir / \
            f"{Path(split_filename).stem}_extracted.json"
        corrected_json = gemini_dir / \
            f"{Path(split_filename).stem}_corrected.json"

        # Output directories debugging
        print(f"üìÇ Extractions Output Dir: {extractions_dir}")
        print(f"üìÇ Extractions Dir Exists: {extractions_dir.exists()}")
        print(f"ü§ñ Gemini Output Dir: {gemini_dir}")
        print(f"ü§ñ Gemini Dir Exists: {gemini_dir.exists()}")
        print(f"üìÑ Extracted JSON Path: {extracted_json}")
        print(f"ü§ñ Corrected JSON Path: {corrected_json}")

        # Show relative paths for clarity
        try:
            cwd = Path.cwd()
            print(f"üìÅ Current Working Directory: {cwd}")
            print(
                f"üìã Template Relative Path: {template_path.relative_to(cwd)}")
            print(
                f"üìÑ Split PDF Relative Path: {Path(split_path).relative_to(cwd)}")
            print(
                f"üìÑ Extracted JSON Relative Path: {extracted_json.relative_to(cwd)}")
            print(
                f"ü§ñ Corrected JSON Relative Path: {corrected_json.relative_to(cwd)}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not compute relative paths: {e}")

        print(f"=== END FILE PATHS DEBUG ===\n")

        print(
            f"Selected template: {template_name} (company={template_entry['company']}) for form_code={form_code}")

        if not template_path.exists():
            raise HTTPException(
                status_code=404, detail=f"Resolved template path not found: {template_path}")

        print(f"Final template being used: {template_name}")

        # Validate critical paths before proceeding
        if not template_path.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Template file not found: {template_path}"
            )

        if not Path(split_path).exists():
            raise HTTPException(
                status_code=404,
                detail=f"Split PDF file not found: {split_path}"
            )

        print(f"‚úÖ All critical files exist and are accessible")

        # Step 1: Run PDF extraction
        extraction_cmd = [
            sys.executable,
            "services/pdf_splitted_extraction.py",
            "--template", str(template_path),
            "--pdf", split_path,
            "--output", str(extracted_json)
        ]

        print(f"\nüîß === EXTRACTION COMMAND DEBUG ===")
        print(f"Command: {' '.join(extraction_cmd)}")
        print(f"Working Directory: {Path.cwd()}")
        print(f"Python Executable: {sys.executable}")
        print(
            f"Extraction Script Exists: {Path('services/pdf_splitted_extraction.py').exists()}")
        print(f"Template Arg: --template {template_path}")
        print(f"PDF Arg: --pdf {split_path}")
        print(f"Output Arg: --output {extracted_json}")
        print(f"üìÅ Template path exists: {template_path.exists()}")
        print(f"üìÑ Split path exists: {Path(split_path).exists()}")
        print(f"üìÇ Extractions dir exists: {extractions_dir.exists()}")
        print(f"üìÇ Gemini dir exists: {gemini_dir.exists()}")
        print(f"=== END COMMAND DEBUG ===\n")

        # Add extraction timeout and better error handling
        extraction_result = subprocess.run(
            # 2 min timeout for basic extraction
            extraction_cmd, capture_output=True, text=True, timeout=120)

        print(f"üíØ Extraction return code: {extraction_result.returncode}")
        print(f"üì§ Extraction stdout: {extraction_result.stdout}")
        if extraction_result.stderr:
            print(f"‚ùå Extraction stderr: {extraction_result.stderr}")

        if extraction_result.returncode != 0:
            error_msg = extraction_result.stderr or "Unknown extraction error"
            print(
                f"‚ùå Extraction failed with code {extraction_result.returncode}")
            raise HTTPException(
                status_code=500,
                detail=f"Extraction failed: {error_msg}"
            )

        # Check if extracted JSON is empty or has minimal data
        extracted_is_empty = False
        extracted_row_count = 0

        if extracted_json.exists():
            try:
                with open(extracted_json, "r", encoding="utf-8") as ef:
                    extracted_data = json.load(ef)
                    if isinstance(extracted_data, list):
                        extracted_row_count = sum(
                            len(page.get("Rows", [])) for page in extracted_data)
                    else:
                        extracted_row_count = len(
                            extracted_data.get("Rows", []))

                    if extracted_row_count < 5:  # Consider less than 5 rows as "empty"
                        extracted_is_empty = True
                        print(
                            f"‚ö†Ô∏è Extracted JSON has minimal data ({extracted_row_count} rows) - will force Gemini correction")
                    else:
                        print(
                            f"‚úÖ Extracted JSON has {extracted_row_count} rows")
            except Exception as e:
                extracted_is_empty = True
                print(
                    f"‚ö†Ô∏è Error reading extracted JSON: {e} - will force Gemini correction")
        else:
            extracted_is_empty = True
            print(f"‚ö†Ô∏è Extracted JSON not found - will force Gemini correction")

        # Step 2: Smart Gemini correction with quick mode option
        import os  # ensure os available for env config

        # PERFORMANCE OPTIMIZATION: Quick mode for small, well-extracted datasets
        enable_quick_mode = os.getenv("GEMINI_QUICK_MODE", "0") == "1"
        quick_mode_threshold = int(
            os.getenv("GEMINI_QUICK_MODE_THRESHOLD", "20"))

        # Skip Gemini for small, well-extracted datasets in quick mode
        skip_gemini = (enable_quick_mode and
                       not extracted_is_empty and
                       extracted_row_count > 0 and
                       extracted_row_count <= quick_mode_threshold)

        if skip_gemini:
            print(
                f"‚ö° QUICK MODE: Skipping Gemini correction for small dataset ({extracted_row_count} rows)")
            gemini_corrected = False
            final_json_path = extracted_json
            correction_notes = {
                "quick_mode_used": True,
                "rows_processed": extracted_row_count,
                "reason": "skipped_for_speed"
            }
        else:
            print(
                f"ü§ñ Starting Gemini correction (empty extraction: {extracted_is_empty}, rows: {extracted_row_count})")

            # CRITICAL: Verify environment is loaded correctly
            api_key_check = os.getenv("GEMINI_API_KEY")
            if not api_key_check:
                print("‚ùå CRITICAL: GEMINI_API_KEY not found in environment!")
                print("üîß Attempting to reload .env file...")
                load_dotenv()
                api_key_check = os.getenv("GEMINI_API_KEY")
                if api_key_check:
                    print(
                        f"‚úÖ GEMINI_API_KEY loaded after reload (length: {len(api_key_check)})")
                else:
                    print("‚ùå GEMINI_API_KEY still not found after reload!")
            else:
                print(f"‚úÖ GEMINI_API_KEY found (length: {len(api_key_check)})")

            # OPTIMIZED: Faster timeouts for quicker processing
            # Shorter timeouts since we're using smaller batches and multithreading
            primary_timeout_env = os.getenv(
                "GEMINI_CORRECTION_TIMEOUT_PRIMARY", "300")   # Reduced from 180s
            retry_timeout_env = os.getenv(
                "GEMINI_CORRECTION_TIMEOUT_RETRY", "200")  # Reduced from 120s
            no_timeout_mode = os.getenv(
                "GEMINI_CORRECTION_NO_TIMEOUT", "0") == "1"
            primary_timeout = None if no_timeout_mode else int(
                primary_timeout_env)
            retry_timeout = None if no_timeout_mode else int(retry_timeout_env)
            enable_retry = os.getenv("GEMINI_CORRECTION_RETRY", "1") != "0"
            enable_second_retry = os.getenv(
                "GEMINI_CORRECTION_SECOND_RETRY", "1") != "0"

            # OPTIMIZED: Faster batch sizes and better multithreading defaults
            dynamic_rows = extracted_row_count if not extracted_is_empty else 0

            # Smaller batches for faster parallel processing
            if dynamic_rows > 200:
                initial_batch = 25  # Reduced from 18 for better parallelization
            elif dynamic_rows > 100:
                initial_batch = 20  # Reduced from 12
            elif dynamic_rows > 50:
                initial_batch = 15  # Reduced from 6
            else:
                initial_batch = 10  # Reduced from 6 for faster small datasets

            # Allow override via env (with better defaults)
            initial_batch = int(
                os.getenv("GEMINI_CORRECTION_INITIAL_BATCH", str(initial_batch)))
            second_batch = int(
                # Faster retry
                os.getenv("GEMINI_CORRECTION_SECOND_BATCH", "8"))
            # Fastest final retry
            third_batch = int(os.getenv("GEMINI_CORRECTION_THIRD_BATCH", "5"))

            # Force enable multithreading and optimize worker count
            max_workers = min(8, max(2, dynamic_rows // 10)
                              )  # Auto-scale workers
            max_workers = int(os.getenv("GEMINI_WORKERS", str(max_workers)))

            print(f"üöÄ PERFORMANCE OPTIMIZATION:")
            print(f"   Rows to process: {dynamic_rows}")
            print(f"   Initial batch size: {initial_batch}")
            print(f"   Max workers: {max_workers}")
            print(f"   Multithreading: ENABLED")

            def run_gemini():
                cmd = [
                    sys.executable,
                    "services/pdf_splitted_gemini_very.py",
                    "--template", str(template_path),
                    "--extracted", str(extracted_json),
                    "--pdf", split_path,
                    "--output", str(corrected_json)
                    # Optionally: "--model", "gemini-2.5-flash"
                ]
                print(f"Prompt being sent to Google Gemini : {cmd}")
                # Enhanced Gemini command debugging
                print(f"\nü§ñ === GEMINI CORRECTION PATHS DEBUG ===")
                print(f"üîß Template Path: {template_path}")
                print(f"üìÑ Extracted JSON: {extracted_json}")
                print(f"üìÑ Extracted JSON Exists: {extracted_json.exists()}")
                print(f"üìÑ Split PDF Path: {split_path}")
                print(f"ü§ñ Corrected JSON Output: {corrected_json}")
                print(f"=== END GEMINI PATHS DEBUG ===\n")
                try:
                    result = subprocess.run(
                        cmd, capture_output=True, text=True, timeout=primary_timeout)
                    return result, None
                except subprocess.TimeoutExpired as te:
                    print(
                        f"‚è∞ Gemini correction timed out after {primary_timeout}s")
                    return subprocess.CompletedProcess(cmd, 124, "", f"Timeout after {primary_timeout}s"), te

            # Only one call needed for single-call Gemini
            correction_result, primary_timeout_exc = run_gemini()
            # Remove all retry logic for Gemini correction (single call only)
            retry_used = False
            second_retry_used = False

            print(
                f"üíØ Gemini correction final return code: {correction_result.returncode}")
            if correction_result.stdout:
                print(
                    f"üì§ Gemini correction stdout (final): {correction_result.stdout[:2000]}")
            if correction_result.stderr:
                print(
                    f"‚ùå Gemini correction stderr (final): {correction_result.stderr[:2000]}")

            gemini_corrected = False
            correction_notes = {
                "primary_timeout_sec": primary_timeout if primary_timeout is not None else "none",
                "retry_timeout_sec": retry_timeout if retry_timeout is not None else "none",
                "retry_used": retry_used,
                "second_retry_used": second_retry_used,
                "primary_timed_out": primary_timeout_exc is not None,
                "attempt_return_code": correction_result.returncode,
                "initial_batch": initial_batch,
                "second_batch": second_batch,
                "third_batch": third_batch,
                "no_timeout_mode": no_timeout_mode
            }

            final_json_path = extracted_json  # Default fallback

            if correction_result.returncode == 0 and corrected_json.exists():
                try:
                    with open(corrected_json, "r", encoding="utf-8") as cf:
                        corrected_content = cf.read()
                        corrected_data = json.loads(corrected_content)
                    # Enhanced row counting to handle multiple JSON formats

                    def count_rows(obj):
                        """Count rows in various JSON formats from different versions of Gemini correction"""
                        if obj is None:
                            return 0

                        # Format 1: New Gemini format with metadata wrapper
                        if isinstance(obj, dict) and "data" in obj:
                            data = obj["data"]
                            if isinstance(data, list):
                                total = 0
                                for item in data:
                                    if isinstance(item, dict) and "Rows" in item:
                                        rows = item["Rows"]
                                        if isinstance(rows, list):
                                            total += len(rows)
                                    elif isinstance(item, list):
                                        total += len(item)
                                return total
                            return 0

                        # Format 2: Direct list format (old extraction)
                        if isinstance(obj, list):
                            total = 0
                            for item in obj:
                                if isinstance(item, dict):
                                    # Check for "Rows" key
                                    if "Rows" in item:
                                        rows = item["Rows"]
                                        if isinstance(rows, list):
                                            total += len(rows)
                                    # Check for "data" key
                                    elif "data" in item:
                                        data = item["data"]
                                        if isinstance(data, list):
                                            total += len(data)
                                elif isinstance(item, list):
                                    total += len(item)
                            return total

                        # Format 3: Direct dict with "Rows"
                        if isinstance(obj, dict):
                            if "Rows" in obj:
                                rows = obj["Rows"]
                                if isinstance(rows, list):
                                    return len(rows)
                            # Check for "data" key at root level
                            elif "data" in obj:
                                data = obj["data"]
                                if isinstance(data, list):
                                    return len(data)

                        return 0
                    corrected_row_count = count_rows(corrected_data)

                    # Enhanced debugging for corrected JSON structure
                    print(f"üîç DEBUG: Corrected JSON structure analysis:")
                    print(f"   Type: {type(corrected_data)}")
                    if isinstance(corrected_data, dict):
                        print(f"   Keys: {list(corrected_data.keys())}")
                        if "data" in corrected_data:
                            data = corrected_data["data"]
                            print(f"   data type: {type(data)}")
                            if isinstance(data, list) and data:
                                print(f"   data length: {len(data)}")
                                print(f"   first item type: {type(data[0])}")
                                if isinstance(data[0], dict):
                                    print(
                                        f"   first item keys: {list(data[0].keys())}")
                    elif isinstance(corrected_data, list):
                        print(f"   List length: {len(corrected_data)}")
                        if corrected_data:
                            print(
                                f"   First item type: {type(corrected_data[0])}")
                            if isinstance(corrected_data[0], dict):
                                print(
                                    f"   First item keys: {list(corrected_data[0].keys())}")

                    print(
                        f"üî¢ Calculated corrected row count: {corrected_row_count}")

                    extracted_row_count_baseline = 0
                    try:
                        if extracted_json.exists():
                            with open(extracted_json, "r", encoding="utf-8") as ef:
                                baseline = json.load(ef)
                            extracted_row_count_baseline = count_rows(baseline)
                            print(
                                f"üî¢ Calculated baseline row count: {extracted_row_count_baseline}")
                    except Exception as baseline_e:
                        print(f"‚ö†Ô∏è Error calculating baseline: {baseline_e}")
                        pass
                    # More lenient success criteria for Gemini correction
                    # Accept 0 or more rows (Gemini might clean up sparse data)
                    if corrected_row_count >= 0:
                        gemini_corrected = True
                        final_json_path = corrected_json

                        if corrected_row_count > 0:
                            print(
                                f"‚úÖ Gemini correction successful: {corrected_row_count} rows (baseline {extracted_row_count_baseline})")
                            if corrected_row_count <= extracted_row_count_baseline:
                                print(
                                    "‚ÑπÔ∏è Corrected row count not higher than baseline ‚Äì may include qualitative normalization / header fixes")
                        else:
                            print(
                                f"‚úÖ Gemini correction successful: Data cleaned/optimized (0 rows, baseline {extracted_row_count_baseline})")
                            print(
                                "‚ÑπÔ∏è Gemini may have cleaned up sparse or redundant data")
                    else:
                        print(
                            "‚ö†Ô∏è Corrected JSON has invalid structure ‚Äì discarding and using extracted data")
                except Exception as e:
                    print(
                        f"‚ö†Ô∏è Error validating corrected JSON: {e} ‚Äì using extracted data")
            else:
                print("‚ö†Ô∏è Gemini correction not successful ‚Äì using extracted data")
                print(
                    f"üîß Debug Info: return_code={correction_result.returncode}")
                if correction_result.stdout:
                    print(f"üì§ Stdout: {correction_result.stdout[:1000]}")
                if correction_result.stderr:
                    print(f"‚ùå Stderr: {correction_result.stderr[:1000]}")
                print(f"üîç Corrected JSON exists: {corrected_json.exists()}")
                if corrected_json.exists():
                    try:
                        file_size = corrected_json.stat().st_size
                        print(f"üìä Corrected JSON file size: {file_size} bytes")
                        if file_size > 0:
                            with open(corrected_json, "r", encoding="utf-8") as f:
                                preview = f.read(500)
                                print(f"üìÑ File preview: {preview}")
                    except Exception as pe:
                        print(f"‚ùå Error reading corrected JSON: {pe}")

            # Enhanced / enriched corrected variant preference
            enhanced_corrected_json = Path(str(corrected_json).replace(
                "_corrected.json", "_corrected_enhanced.json"))
            if enhanced_corrected_json.exists():
                try:
                    with open(enhanced_corrected_json, "r", encoding="utf-8") as enf:
                        _ = json.load(enf)  # sanity parse
                    final_json_path = enhanced_corrected_json
                    gemini_corrected = True
                    correction_notes["used_enhanced"] = True
                    print(
                        f"üéØ Using Enhanced Gemini-corrected JSON: {final_json_path}")
                except Exception as ee:
                    print(f"‚ö†Ô∏è Failed to parse enhanced corrected JSON: {ee}")
            else:
                correction_notes["used_enhanced"] = False

            if not gemini_corrected:
                correction_notes["reason"] = "correction_failed_or_empty"
            else:
                correction_notes["reason"] = "success"

        # Load the final JSON
        with open(final_json_path, "r", encoding="utf-8") as f:
            final_data = json.load(f)

        # Perform final validation check
        try:
            validation_file = gemini_dir / \
                f"{Path(split_filename).stem}_validation.json"
            if validation_file.exists():
                with open(validation_file, "r", encoding="utf-8") as vf:
                    validation_report = json.load(vf)
                    if not validation_report.get("is_valid", True):
                        print(
                            f"‚ö†Ô∏è Validation issues detected: {validation_report.get('issues', [])}")
                    else:
                        print(
                            f"‚úÖ Data validation passed with {validation_report['stats']['total_rows']} rows")
            else:
                print("‚ö†Ô∏è No validation report found")
        except Exception as ve:
            print(f"Validation check failed: {ve}")

        # Store metadata about the extraction
        extraction_metadata = {
            "extraction_id": str(uuid.uuid4()),
            "user_id": user_id,
            "company_name": company_name,
            "pdf_name": pdf_name,
            "split_filename": split_filename,
            "split_pdf_path": split_path,  # Add the actual split PDF path used
            "form_code": form_code,
            "template_used": str(template_path),
            "extracted_at": datetime.now().isoformat(),
            "extraction_status": "completed",
            # Use the actual flag from correction process
            "gemini_corrected": gemini_corrected,
            # Use the actual final file path
            "output_path": str(final_json_path),
            "correction_meta": correction_notes
        }

        # Save metadata
        metadata_path = extractions_dir / \
            f"{Path(split_filename).stem}_metadata.json"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(extraction_metadata, f, indent=2, ensure_ascii=False)

        # Normalize final_data to ensure consistent frontend format
        # Handle both old format {metadata, data} and new format {Rows, _metadata}
        normalized_data = final_data
        if isinstance(final_data, dict):
            if "Rows" in final_data:
                # New Gemini format: {Rows: [...], _metadata: {...}}
                # Convert to frontend-expected format: {data: [...]}
                normalized_data = final_data["Rows"]
                print(
                    f"üîÑ Normalized new Gemini format: {len(normalized_data) if isinstance(normalized_data, list) else 0} items")
            elif "data" in final_data:
                # Old Gemini format: {metadata: {...}, data: [...]}
                # Extract the data array
                normalized_data = final_data["data"]
                print(
                    f"üîÑ Normalized old Gemini format: {len(normalized_data) if isinstance(normalized_data, list) else 0} items")
            # If it's already a plain dict or other format, leave as-is

        # --- Store Gemini-verified data in Companies, Reports, ReportData tables ---
        print(f"\nüóÑÔ∏è === DATABASE STORAGE DEBUG ===")
        print(f"üìä Normalized data type: {type(normalized_data)}")
        print(
            f"üìä Normalized data length: {len(normalized_data) if isinstance(normalized_data, list) else 'N/A'}")

        try:
            from databases.models import Company, ReportModels
            from databases.database import SessionLocal
            from sqlalchemy.exc import IntegrityError

            print(f"[DB] Creating database session...")
            db = SessionLocal()

            try:
                # 1. Find or create company
                print(f"[DB] Looking for company: {company_name}")
                company_obj = db.query(Company).filter_by(
                    name=company_name).first()
                if not company_obj:
                    print(f"[DB] Company not found, creating new entry...")
                    company_obj = Company(name=company_name)
                    db.add(company_obj)
                    db.commit()
                    db.refresh(company_obj)
                    print(
                        f"[DB] ‚úÖ Created company with ID: {company_obj.id}")
                else:
                    print(
                        f"[DB] ‚úÖ Found existing company with ID: {company_obj.id}")
            except IntegrityError as ie:
                print(
                    f"‚ö†Ô∏è Company creation integrity error (might already exist): {ie}")
                db.rollback()
                # Try to fetch again after rollback
                company_obj = db.query(Company).filter_by(
                    name=company_name).first()
                if not company_obj:
                    raise HTTPException(
                        status_code=500,
                        detail=f"Failed to create or fetch company: {company_name}"
                    )

            # 2. Group normalized_data by Period and insert separate rows for each unique period
            print(f"[DB] Grouping data by Period for storage...")

            # Group tables by period
            period_groups = {}
            if isinstance(normalized_data, list):
                for table in normalized_data:
                    period = table.get("Period", str(datetime.now().date()))
                    if period not in period_groups:
                        period_groups[period] = []
                    period_groups[period].append(table)

                print(
                    f"[DB] Found {len(period_groups)} unique periods: {list(period_groups.keys())}")

            # If no data, create a single default entry
            if not period_groups:
                period_groups[str(datetime.now().date())] = []
                print(f"[DB] No data found, using current date as period")

            # 1Ô∏è‚É£ Resolve company DB record for foreign key
            company_obj = db.query(Company).filter_by(
                name=company_name).first()
            if not company_obj:
                company_obj = Company(name=company_name)
                db.add(company_obj)
                db.commit()
                db.refresh(company_obj)

            # 2Ô∏è‚É£ Convert company name to table key
            table_key = company_name.lower().replace(" ", "_")
            report_model = ReportModels.get(table_key)

            if not report_model:
                raise HTTPException(
                    status_code=500,
                    detail=f"No table found for company: {company_name}"
                )

            # Extract ReportType (Standalone/Consolidated)
            report_type = None
            match = re.search(r'\s([SC])\s*FY', pdf_name, re.IGNORECASE)

            if match:
                type_code = match.group(1).upper()
                report_type = "Standalone" if type_code == "S" else "Consolidated"
            else:
                report_type = "Standalone"  # Optional fallback

            # 3Ô∏è‚É£ Insert one row per unique period
            report_objects = []
            for report_period, tables_for_period in period_groups.items():
                print(
                    f"[DB] Creating Report entry for period: {report_period}")

                # Combine all rows from tables with same period
                combined_rows = []
                currency = None
                registration_number = None
                title = None
                pages_used = None
                flat_headers = None

                for table in tables_for_period:
                    # Extract metadata from first table in this period group
                    if not currency:
                        currency = table.get("Currency")
                        registration_number = table.get("RegistrationNumber")
                        title = table.get("Title")
                        pages_used = table.get("PagesUsed")
                        flat_headers = table.get("FlatHeaders")

                    # Combine all rows
                    table_rows = table.get("Rows", [])
                    combined_rows.extend(table_rows)

                print(
                    f"[DB] Combined {len(combined_rows)} rows for period: {report_period}")

                try:
                    report_obj = report_model(
                        company=company_name,
                        company_id=company_obj.id,
                        ReportType=report_type,
                        pdf_name=pdf_name,
                        registration_number=registration_number,
                        form_no=form_code,
                        title=title,
                        period=str(report_period),
                        currency=currency,
                        pages_used=pages_used,
                        source_pdf=split_filename,
                        flat_headers=flat_headers,
                        data_rows=combined_rows
                    )

                    db.add(report_obj)
                    db.commit()
                    db.refresh(report_obj)
                    report_objects.append(report_obj)

                    print(
                        f"[DB] ‚úÖ Created Report with ID: {report_obj.id} for period: {report_period}")
                except IntegrityError as ie:
                    print(
                        f"‚ö†Ô∏è Period Master or Report integrity error for period '{report_period}': {ie}")
                    db.rollback()
                    # Try to find existing report instead
                    existing_report = db.query(report_model).filter_by(
                        company_id=company_obj.id,
                        form_no=form_code,
                        period=str(report_period),
                        source_pdf=split_filename
                    ).first()

                    if existing_report:
                        print(
                            f"[DB] ‚ÑπÔ∏è Found existing report with ID: {existing_report.id}, updating data...")
                        # Update existing report
                        existing_report.data_rows = combined_rows
                        existing_report.flat_headers = flat_headers
                        existing_report.pages_used = pages_used
                        db.commit()
                        db.refresh(existing_report)
                        report_objects.append(existing_report)
                        print(
                            f"[DB] ‚úÖ Updated existing Report ID: {existing_report.id}")
                    else:
                        print(
                            f"[DB] ‚ùå Could not create or find report for period: {report_period}")
                        continue

            print(
                f"[DB] ‚úÖ Total {len(report_objects)} Report rows created (one per unique period)")
            # -------------------------------------------------------
            #  EXTRA INSERTION INTO L-FORM TABLE (reports_l1 / l2 / etc.)
            # -------------------------------------------------------
            try:

                def normalize_lform_key(form_code: str) -> str:
                    fc = form_code.upper().strip()

                    # Special cases first
                    special_forms = {
                        r'L[-\s]?6A.*': "l6a",
                        r'L[-\s]?9A.*': "l9a",
                        r'L[-\s]?14A.*': "l14a",
                        r'L[-\s]?25\(I\).*': "l25_i",
                        r'L[-\s]?25\(II\).*': "l25_ii",
                    }
                    for pattern, key in special_forms.items():
                        if re.match(pattern, fc, re.IGNORECASE):
                            return key.lower()

                    # General fallback (L-1, L-8A, L-10B etc)
                    match = re.match(r"L[-\s]?(\d+[A-Z]?)", fc, re.IGNORECASE)
                    if match:
                        base = match.group(1).lower()
                        return f"l{base}"

                    # no match = return as-is
                    return fc.lower()

                # ------------------------------------------------------------------
                # INSERT THIS right before report_model lookup
                # ------------------------------------------------------------------
                # Normalize form code to detect correct L-form table
                lform_key = normalize_lform_key(
                    form_code)  # <= most important change

                print(f"[DB] Normalized L-Form Key: {lform_key}")

                # Prefer L-form table ‚Üí fallback to company table
                table_key = None

                if lform_key in ReportModels:
                    table_key = lform_key  # L-forms first priority
                    print(f"[DB] üöÄ Using L-Form Table: reports_{table_key}")
                else:
                    # Default to company table
                    table_key = company_name.lower().replace(" ", "_")
                    print(f"[DB] üè¢ Using Company Table: reports_{table_key}")

                report_model = ReportModels.get(table_key)
                if not report_model:
                    raise HTTPException(
                        status_code=500,
                        detail=f"No report table found for key: {table_key}"
                    )

                # Store one row per period in L-form table
                if lform_key not in ReportModels:
                    print(
                        f"[DB] ‚ö†Ô∏è No matching L-Form table found for: {form_code}")
                else:
                    lform_model = ReportModels[lform_key]

                    # Insert into L-form table for each period
                    for report_period, tables_for_period in period_groups.items():
                        # Combine rows for this period
                        combined_rows = []
                        currency = None
                        registration_number = None
                        title = None
                        pages_used = None
                        flat_headers = None

                        for table in tables_for_period:
                            if not currency:
                                currency = table.get("Currency")
                                registration_number = table.get(
                                    "RegistrationNumber")
                                title = table.get("Title")
                                pages_used = table.get("PagesUsed")
                                flat_headers = table.get("FlatHeaders")

                            table_rows = table.get("Rows", [])
                            combined_rows.extend(table_rows)

                        try:
                            lform_obj = lform_model(
                                company=company_name,
                                company_id=company_obj.id,
                                ReportType=report_type,
                                pdf_name=pdf_name,
                                registration_number=registration_number,
                                form_no=form_code,
                                title=title,
                                period=str(report_period),
                                currency=currency,
                                pages_used=pages_used,
                                source_pdf=split_filename,
                                flat_headers=flat_headers,
                                data_rows=combined_rows
                            )
                            db.add(lform_obj)
                            db.commit()
                            print(
                                f"[DB] üìå Also stored into L-Form table: {lform_key} for period: {report_period}")
                        except IntegrityError as ie:
                            print(
                                f"‚ö†Ô∏è L-Form table integrity error for period '{report_period}': {ie}")
                            db.rollback()
                            # Try to find and update existing
                            existing_lform = db.query(lform_model).filter_by(
                                company_id=company_obj.id,
                                form_no=form_code,
                                period=str(report_period),
                                source_pdf=split_filename
                            ).first()

                            if existing_lform:
                                print(
                                    f"[DB] ‚ÑπÔ∏è Found existing L-form report, updating data...")
                                existing_lform.data_rows = combined_rows
                                existing_lform.flat_headers = flat_headers
                                db.commit()
                                print(f"[DB] ‚úÖ Updated existing L-Form report")
                            else:
                                print(
                                    f"[DB] ‚ö†Ô∏è Could not create or find L-form report")
            except Exception as lf_exc:
                print(f"‚ùå Failed to store into L-Form table: {lf_exc}")

            # 3. Insert each row into ReportData
            print(
                f"[DB] Attempting to store {len(normalized_data)} rows in reportdata for reportid={report_obj.id}")
            inserted_count = 0
            print(
                f"[DB] ‚úÖ Successfully inserted {inserted_count} rows into reportdata for reportid={report_obj.id}")
            db.close()
            print(f"‚úÖ Stored extraction in companies, reports, reportdata tables.")
            print(f"=== END DATABASE STORAGE DEBUG ===\n")
        except Exception as db_exc:
            import traceback
            print(f"‚ùå Failed to store extraction in DB: {db_exc}")
            print(f"‚ùå Full traceback: {traceback.format_exc()}")
            print(f"=== END DATABASE STORAGE DEBUG ===\n")

        return {
            "success": True,
            "extraction_id": extraction_metadata["extraction_id"],
            "data": normalized_data,
            "metadata": extraction_metadata
        }

    except Exception as e:
        error_msg = str(e)
        print(f"Form extraction error: {error_msg}")
        import traceback
        traceback_str = traceback.format_exc()
        print(f"Full traceback: {traceback_str}")

        # Enhanced error reporting
        if not error_msg:
            error_msg = "Unknown extraction error occurred"

        # Include more context in the error
        context_info = f"Company: {company_name}, PDF: {pdf_name}, Split: {split_filename}"
        detailed_error = f"{error_msg} | Context: {context_info}"

        raise HTTPException(status_code=500, detail=detailed_error)


@router.get("/companies/{company_name}/pdfs/{pdf_name}/form-preferences")
async def get_form_preferences(company_name: str, pdf_name: str):
    """
    Get form visibility preferences for a specific PDF (shared across all users)
    Returns empty array if preferences don't exist (first time)
    """
    try:
        prefs = load_form_preferences()
        key = f"{company_name}_{pdf_name}"

        # Check if key exists in preferences (None means no preferences saved yet)
        if key in prefs:
            enabled_forms = prefs[key]
            # Return the saved preferences (even if empty array)
            return {
                "success": True,
                "data": {
                    "enabled_forms": enabled_forms
                }
            }
        else:
            # No preferences saved yet - return None to indicate first time
            return {
                "success": True,
                "data": {
                    "enabled_forms": None
                }
            }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to get preferences: {str(e)}")


@router.post("/companies/{company_name}/pdfs/{pdf_name}/form-preferences")
async def set_form_preferences(
    company_name: str,
    pdf_name: str,
    request: FormPreferencesRequest
):
    """
    Set form visibility preferences for a specific PDF (admin only, shared across all users)
    """
    try:
        prefs = load_form_preferences()
        key = f"{company_name}_{pdf_name}"
        prefs[key] = request.enabled_forms
        save_form_preferences(prefs)

        return {
            "success": True,
            "message": f"Form preferences saved for {company_name}/{pdf_name}",
            "data": {
                "enabled_forms": request.enabled_forms
            }
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to save preferences: {str(e)}")


# Data edits storage (using JSON file for simplicity)
DATA_EDITS_FILE = BASE_DIR / "data_edits.json"

# Pydantic models for data edits


class CellEditRequest(BaseModel):
    form_name: str
    record_index: int
    row_index: int
    header: str
    value: str


class BulkEditRequest(BaseModel):
    edits: List[CellEditRequest]


def load_data_edits() -> Dict:
    """Load data edits from JSON file"""
    if DATA_EDITS_FILE.exists():
        try:
            with open(DATA_EDITS_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading data edits: {e}")
            return {}
    return {}


def save_data_edits(edits: Dict):
    """Save data edits to JSON file"""
    try:
        with open(DATA_EDITS_FILE, 'w') as f:
            json.dump(edits, f, indent=2)
    except Exception as e:
        print(f"Error saving data edits: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to save edits: {str(e)}")


@router.get("/companies/{company_name}/pdfs/{pdf_name}/data-edits")
async def get_data_edits(company_name: str, pdf_name: str):
    """
    Get all data edits for a specific PDF (shared across all users)
    """
    try:
        edits = load_data_edits()
        key = f"{company_name}_{pdf_name}"
        pdf_edits = edits.get(key, {})

        return {
            "success": True,
            "data": {
                "edits": pdf_edits
            }
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to get edits: {str(e)}")


@router.post("/companies/{company_name}/pdfs/{pdf_name}/data-edits")
async def save_data_edit(
    company_name: str,
    pdf_name: str,
    request: CellEditRequest
):
    """
    Save a single cell edit (admin only, shared across all users)
    """
    try:
        edits = load_data_edits()
        key = f"{company_name}_{pdf_name}"

        if key not in edits:
            edits[key] = {}

        # Create edit key: form_recordIndex_rowIndex_header
        edit_key = f"{request.form_name}_{request.record_index}_{request.row_index}_{request.header}"
        edits[key][edit_key] = {
            "form_name": request.form_name,
            "record_index": request.record_index,
            "row_index": request.row_index,
            "header": request.header,
            "value": request.value,
            "edited_at": datetime.now().isoformat()
        }

        save_data_edits(edits)

        return {
            "success": True,
            "message": f"Cell edit saved for {company_name}/{pdf_name}",
            "data": {
                "edit": edits[key][edit_key]
            }
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to save edit: {str(e)}")


@router.delete("/companies/{company_name}/pdfs/{pdf_name}/data-edits")
async def delete_data_edit(
    company_name: str,
    pdf_name: str,
    form_name: str,
    record_index: int,
    row_index: int,
    header: str
):
    """
    Delete a specific cell edit (admin only)
    """
    try:
        edits = load_data_edits()
        key = f"{company_name}_{pdf_name}"

        if key in edits:
            edit_key = f"{form_name}_{record_index}_{row_index}_{header}"
            if edit_key in edits[key]:
                del edits[key][edit_key]
                save_data_edits(edits)

        return {
            "success": True,
            "message": "Edit deleted successfully"
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to delete edit: {str(e)}")


@router.get("/companies/{company_name}/pdfs/{pdf_name}/splits/{split_filename}/extraction")
async def get_extracted_data(company_name: str, pdf_name: str, split_filename: str):
    """
    Get previously extracted data for a split - checks both extractions and gemini_verified_json directories
    """
    try:
        # Helper function to find company directory with flexible naming
        def find_company_dir(base_path: Path, company_name: str):
            # Create both underscore and spaced versions
            company_slug = company_name.lower().replace(" ", "_")
            company_spaced = company_name.lower().replace("_", " ")

            # Try underscore version first
            underscore_path = base_path / company_slug
            if underscore_path.exists():
                return underscore_path

            # Try spaced version
            spaced_path = base_path / company_spaced
            if spaced_path.exists():
                return spaced_path

            # Try original input as-is
            original_path = base_path / company_name.lower()
            if original_path.exists():
                return original_path

            # If none exist, return the underscore version for consistency
            return underscore_path

        # Find company directories with flexible naming
        gemini_company_dir = find_company_dir(
            Path("gemini_verified_json"), company_name)
        extractions_company_dir = find_company_dir(
            Path("extractions"), company_name)

        # Check Gemini-verified directory first (highest priority)
        gemini_dir = gemini_company_dir / pdf_name
        gemini_corrected_json = gemini_dir / \
            f"{Path(split_filename).stem}_corrected.json"
        gemini_metadata_path = gemini_dir / \
            f"{Path(split_filename).stem}_metadata.json"

        # Check original extractions directory
        extractions_dir = extractions_company_dir / pdf_name
        corrected_json = extractions_dir / \
            f"{Path(split_filename).stem}_corrected.json"
        extracted_json = extractions_dir / \
            f"{Path(split_filename).stem}_extracted.json"
        metadata_path = extractions_dir / \
            f"{Path(split_filename).stem}_metadata.json"

        # Priority order: Gemini-verified > corrected > extracted
        json_path = None
        metadata_source = None
        source_type = None

        if gemini_corrected_json.exists():
            json_path = gemini_corrected_json
            metadata_source = gemini_metadata_path if gemini_metadata_path.exists() else metadata_path
            source_type = "gemini_verified"
            print(f"üìã Using Gemini-verified JSON: {json_path}")
        elif corrected_json.exists():
            json_path = corrected_json
            metadata_source = metadata_path
            source_type = "corrected"
            print(f"üìã Using corrected JSON: {json_path}")
        elif extracted_json.exists():
            json_path = extracted_json
            metadata_source = metadata_path
            source_type = "extracted"
            print(f"üìã Using extracted JSON: {json_path}")

        if not json_path:
            return {"success": False, "message": "No extraction data found"}

        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        metadata = {}
        if metadata_source and metadata_source.exists():
            with open(metadata_source, "r", encoding="utf-8") as f:
                metadata = json.load(f)

        # CRITICAL FIX: Force gemini_corrected = True when source is gemini_verified
        if source_type == "gemini_verified":
            metadata["gemini_corrected"] = True
            print(f"ü§ñ Forced gemini_corrected=True for gemini_verified source")
        elif source_type == "corrected":
            # Also treat corrected JSONs as gemini corrected
            metadata["gemini_corrected"] = True
            print(f"ü§ñ Forced gemini_corrected=True for corrected source")

        # HEADER CONSISTENCY FIX: Ensure FlatHeaders match actual row keys (BEFORE normalization for old format)
        if isinstance(data, list) and len(data) > 0:
            for record_idx, record in enumerate(data):
                if isinstance(record, dict) and "Rows" in record and len(record["Rows"]) > 0:
                    # Get actual keys from first row
                    first_row = record["Rows"][0]
                    if isinstance(first_row, dict):
                        actual_keys = list(first_row.keys())

                        # Update FlatHeaders to match actual data
                        record["FlatHeaders"] = actual_keys
                        record["FlatHeadersNormalized"] = actual_keys

                        print(
                            f"üìä Fixed headers for record {record_idx}: {actual_keys}")

        # Normalize data to ensure consistent frontend format
        # Handle both old format {metadata, data} and new format {Rows, _metadata}
        normalized_data = data
        if isinstance(data, dict):
            if "Rows" in data:
                # New Gemini format: {Rows: [...], _metadata: {...}}
                normalized_data = data["Rows"]
                print(
                    f"üîÑ Normalized new Gemini format: {len(normalized_data) if isinstance(normalized_data, list) else 0} items")
            elif "data" in data:
                # Old Gemini format: {metadata: {...}, data: [...]}
                normalized_data = data["data"]
                print(
                    f"üîÑ Normalized old Gemini format: {len(normalized_data) if isinstance(normalized_data, list) else 0} items")
            # If it's already a plain dict or other format, leave as-is

        # HEADER CONSISTENCY FIX: Ensure FlatHeaders match actual row keys (AFTER normalization for new format)
        if isinstance(normalized_data, list) and len(normalized_data) > 0:
            for record_idx, record in enumerate(normalized_data):
                if isinstance(record, dict) and "Rows" in record and len(record["Rows"]) > 0:
                    # Get actual keys from first row
                    first_row = record["Rows"][0]
                    if isinstance(first_row, dict):
                        actual_keys = list(first_row.keys())

                        # Update FlatHeaders to match actual data
                        record["FlatHeaders"] = actual_keys
                        record["FlatHeadersNormalized"] = actual_keys

                        print(
                            f"üîß Fixed headers for normalized record {record_idx}: {actual_keys}")

        return {
            "success": True,
            "data": normalized_data,
            "metadata": metadata,
            "source": source_type,
            "file_path": str(json_path)
        }

    except Exception as e:
        print(f"Get extraction error: {e}")
        return {"success": False, "error": str(e)}


@router.get("/debug-gemini-config")
async def debug_gemini_config():
    """
    Debug endpoint to show current Gemini performance configuration
    """
    import os

    config = {
        "quick_mode": {
            "enabled": os.getenv("GEMINI_QUICK_MODE", "0") == "1",
            "threshold": int(os.getenv("GEMINI_QUICK_MODE_THRESHOLD", "20"))
        },
        "multithreading": {
            "enabled": True,  # Always enabled now
            "workers": int(os.getenv("GEMINI_WORKERS", "4"))
        },
        "timeouts": {
            "primary": int(os.getenv("GEMINI_CORRECTION_TIMEOUT_PRIMARY", "90")),
            "retry": int(os.getenv("GEMINI_CORRECTION_TIMEOUT_RETRY", "60")),
            "third": int(os.getenv("GEMINI_CORRECTION_THIRD_TIMEOUT", "45")),
            "no_timeout_mode": os.getenv("GEMINI_CORRECTION_NO_TIMEOUT", "0") == "1"
        },
        "batch_sizes": {
            "initial": int(os.getenv("GEMINI_CORRECTION_INITIAL_BATCH", "10")),
            "second": int(os.getenv("GEMINI_CORRECTION_SECOND_BATCH", "8")),
            "third": int(os.getenv("GEMINI_CORRECTION_THIRD_BATCH", "5"))
        },
        "retry_settings": {
            "enable_retry": os.getenv("GEMINI_CORRECTION_RETRY", "1") != "0",
            "enable_second_retry": os.getenv("GEMINI_CORRECTION_SECOND_RETRY", "1") != "0"
        },
        "pdf_optimization": {
            "max_pages": "3",  # Fixed for performance
            "retries": "2"     # Fixed for performance
        }
    }

    return {
        "success": True,
        "message": "Current Gemini performance configuration",
        "config": config,
        "recommendations": {
            "for_speed": "Enable GEMINI_QUICK_MODE=1 for small datasets",
            "for_accuracy": "Disable quick mode and increase batch sizes",
            "for_production": "Use 6-8 workers and quick mode enabled"
        }
    }


@router.post("/test-extraction-performance")
async def test_extraction_performance(
    company_name: str = "SBI Life",
    form_code: str = "L-4-PREMIUM",
    simulate_rows: int = 15
):
    """
    Test endpoint to simulate extraction performance and demonstrate quick mode
    """
    import os
    import time

    start_time = time.time()

    # Simulate the same logic as the real extraction
    enable_quick_mode = os.getenv("GEMINI_QUICK_MODE", "0") == "1"
    quick_mode_threshold = int(os.getenv("GEMINI_QUICK_MODE_THRESHOLD", "20"))

    extracted_is_empty = simulate_rows == 0
    extracted_row_count = simulate_rows

    skip_gemini = (enable_quick_mode and
                   not extracted_is_empty and
                   extracted_row_count > 0 and
                   extracted_row_count <= quick_mode_threshold)

    performance_config = {
        "dynamic_rows": extracted_row_count,
        "initial_batch": 10 if extracted_row_count > 50 else 3,
        "max_workers": min(8, max(2, extracted_row_count // 10)),
        "quick_mode_enabled": enable_quick_mode,
        "quick_mode_threshold": quick_mode_threshold,
        "would_skip_gemini": skip_gemini
    }

    # Simulate processing time
    if skip_gemini:
        simulated_time = 2.0  # Very fast without Gemini
        processing_path = "extraction_only"
    else:
        # Simulate Gemini processing time based on batch size and workers
        # 5 seconds per batch item
        batch_processing_time = performance_config["initial_batch"] * 5
        # More workers = faster
        worker_multiplier = 1.0 / performance_config["max_workers"]
        simulated_time = batch_processing_time * worker_multiplier
        processing_path = "extraction_plus_gemini"

    # Simulate some processing time (scaled down)
    time.sleep(min(simulated_time / 10, 1.0))

    end_time = time.time()
    actual_time = end_time - start_time

    return {
        "success": True,
        "test_scenario": {
            "company": company_name,
            "form_code": form_code,
            "simulated_rows": simulate_rows
        },
        "performance_analysis": {
            "processing_path": processing_path,
            "would_skip_gemini": skip_gemini,
            "estimated_time_seconds": round(simulated_time, 2),
            "actual_test_time": round(actual_time, 3),
            "time_savings": round(max(0, 30 - simulated_time), 2) if skip_gemini else 0
        },
        "configuration": performance_config,
        "recommendations": [
            "Enable GEMINI_QUICK_MODE=1 for datasets under 20 rows",
            "Use 6+ workers for large datasets",
            "Consider batch size 3-4 for optimal parallelization"
        ]
    }
